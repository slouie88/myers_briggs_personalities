{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Initial Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset\n",
    "\n",
    "This dataset contains over 8600 rows of data, on each row is a person’s:\n",
    "- Type (This persons 4 letter MBTI code/type)\n",
    "- A section of each of the last 50 things they have posted (Each entry separated by \"|||\" (3 pipe characters))\n",
    "\n",
    "The dataset was taken from Kaggle, but the data itself was collected through the PersonalityCafe forum (http://personalitycafe.com/forum/). The purpose of this dataset is to help see if any patterns can be detected in specific types and their style of writing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "CSV_DATA_PATH = os.path.join(\"data\", \"mbti_1.csv\")\n",
    "\n",
    "def load_csv_data(csv_file_path: str = CSV_DATA_PATH) -> pd.DataFrame:\n",
    "    \"\"\" Load data from a given csv file into a pandas DataFrame object.\n",
    "\n",
    "    Args:\n",
    "    - csv_file_path (str) - The file path of the csv file containing the desired data to load into a pandas DataFrame object.\n",
    "\n",
    "    Returns:\n",
    "    - data (pandas.DataFrame) - The loaded data as a pandas DataFrame object.\n",
    "    \"\"\"\n",
    "    assert csv_file_path.endswith(\".csv\")\n",
    "\n",
    "    return pd.read_csv(csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                              posts\n",
       "0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n",
       "1  ENTP  'I'm finding the lack of me in these posts ver...\n",
       "2  INTP  'Good one  _____   https://www.youtube.com/wat...\n",
       "3  INTJ  'Dear INTP,   I enjoyed our conversation the o...\n",
       "4  ENTJ  'You're fired.|||That's another silly misconce..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = load_csv_data()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8675 entries, 0 to 8674\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   type    8675 non-null   object\n",
      " 1   posts   8675 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 135.7+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Quick check for duplicated rows\n",
    "duplication_bool_series = data.duplicated()\n",
    "duplication_bool_series.sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the newly engineered features, we will build a classifier using a tree-ensemble model like XGBoost and use its feature importance capability to get a sense of which features tend to be most important for determining the 4 axes of personality factors. The significance of these feature importances across the 4 axes of personality factors is, however, dependent on whether a satisfactory model can be fitted well to the given data.\n",
    "\n",
    "For the classifiers to be trained, F1 score will be used as the scoring metric instead of accuracy. This is due to the class imbalances discovered in the data set during initial analysis. As F1 score is the harmonic mean of recall and precision, it takes both metrics into consideration. F1 score can be low as a result of low recall (which can happen in imbalanced data).\n",
    "\n",
    "Stratified k-fold cross validation will also be used for training the classifiers as it enforces a particular ratio of classes for each split. This means that for each split, even if we do have an imbalanced data set, there will still be a particular proportion of classes belonging to the under-represented class for model training.\n",
    "\n",
    "EDA will then be performed on the features that tend to be important in order to gain further insight as to how someone's writing style might relate to their MBTI personality type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>e_vs_i</th>\n",
       "      <th>s_vs_n</th>\n",
       "      <th>t_vs_f</th>\n",
       "      <th>j_vs_p</th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "      <th>url_count</th>\n",
       "      <th>posts_no_url</th>\n",
       "      <th>posts_no_url_tokenized</th>\n",
       "      <th>posts_no_url_pos_tagged</th>\n",
       "      <th>...</th>\n",
       "      <th>adjective_freq</th>\n",
       "      <th>adverb_freq</th>\n",
       "      <th>preposition_freq</th>\n",
       "      <th>interjection_freq</th>\n",
       "      <th>determiner_freq</th>\n",
       "      <th>negative_sentiment</th>\n",
       "      <th>neutral_sentiment</th>\n",
       "      <th>positive_sentiment</th>\n",
       "      <th>overall_sentiment</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>J</td>\n",
       "      <td>INFJ</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw</td>\n",
       "      <td>1</td>\n",
       "      <td>'</td>\n",
       "      <td>[\"'\"]</td>\n",
       "      <td>[(\"'\", \"''\")]</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>J</td>\n",
       "      <td>INFJ</td>\n",
       "      <td>http://41.media.tumblr.com/tumblr_lfouy03PMA1q...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>J</td>\n",
       "      <td>INFJ</td>\n",
       "      <td>enfp and intj moments  https://www.youtube.com...</td>\n",
       "      <td>2</td>\n",
       "      <td>enfp and intj moments    sportscenter not top ...</td>\n",
       "      <td>['enfp', 'and', 'intj', 'moments', 'sportscent...</td>\n",
       "      <td>[('enfp', 'NN'), ('and', 'CC'), ('intj', 'JJ')...</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.4003</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>J</td>\n",
       "      <td>INFJ</td>\n",
       "      <td>What has been the most life-changing experienc...</td>\n",
       "      <td>0</td>\n",
       "      <td>What has been the most life-changing experienc...</td>\n",
       "      <td>['what', 'has', 'been', 'the', 'most', 'life-c...</td>\n",
       "      <td>[('what', 'WDT'), ('has', 'VBZ'), ('been', 'VB...</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>J</td>\n",
       "      <td>INFJ</td>\n",
       "      <td>http://www.youtube.com/watch?v=vXZeYwwRDw8   h...</td>\n",
       "      <td>2</td>\n",
       "      <td>On repeat for most of today.</td>\n",
       "      <td>['on', 'repeat', 'for', 'most', 'of', 'today',...</td>\n",
       "      <td>[('on', 'IN'), ('repeat', 'NN'), ('for', 'IN')...</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  e_vs_i s_vs_n t_vs_f j_vs_p  type  \\\n",
       "0      I      N      F      J  INFJ   \n",
       "1      I      N      F      J  INFJ   \n",
       "2      I      N      F      J  INFJ   \n",
       "3      I      N      F      J  INFJ   \n",
       "4      I      N      F      J  INFJ   \n",
       "\n",
       "                                               posts  url_count  \\\n",
       "0        'http://www.youtube.com/watch?v=qsXHcwe3krw          1   \n",
       "1  http://41.media.tumblr.com/tumblr_lfouy03PMA1q...          1   \n",
       "2  enfp and intj moments  https://www.youtube.com...          2   \n",
       "3  What has been the most life-changing experienc...          0   \n",
       "4  http://www.youtube.com/watch?v=vXZeYwwRDw8   h...          2   \n",
       "\n",
       "                                        posts_no_url  \\\n",
       "0                                                  '   \n",
       "1                                                NaN   \n",
       "2  enfp and intj moments    sportscenter not top ...   \n",
       "3  What has been the most life-changing experienc...   \n",
       "4                       On repeat for most of today.   \n",
       "\n",
       "                              posts_no_url_tokenized  \\\n",
       "0                                              [\"'\"]   \n",
       "1                                                 []   \n",
       "2  ['enfp', 'and', 'intj', 'moments', 'sportscent...   \n",
       "3  ['what', 'has', 'been', 'the', 'most', 'life-c...   \n",
       "4  ['on', 'repeat', 'for', 'most', 'of', 'today',...   \n",
       "\n",
       "                             posts_no_url_pos_tagged  ...  adjective_freq  \\\n",
       "0                                      [(\"'\", \"''\")]  ...               0   \n",
       "1                                                 []  ...               0   \n",
       "2  [('enfp', 'NN'), ('and', 'CC'), ('intj', 'JJ')...  ...               1   \n",
       "3  [('what', 'WDT'), ('has', 'VBZ'), ('been', 'VB...  ...               1   \n",
       "4  [('on', 'IN'), ('repeat', 'NN'), ('for', 'IN')...  ...               1   \n",
       "\n",
       "   adverb_freq  preposition_freq  interjection_freq  determiner_freq  \\\n",
       "0            0                 0                  0                0   \n",
       "1            0                 0                  0                0   \n",
       "2            1                 0                  0                0   \n",
       "3            1                 1                  0                2   \n",
       "4            0                 3                  0                0   \n",
       "\n",
       "   negative_sentiment  neutral_sentiment  positive_sentiment  \\\n",
       "0                0.00               1.00                 0.0   \n",
       "1                0.00               1.00                 0.0   \n",
       "2                0.27               0.73                 0.0   \n",
       "3                0.00               1.00                 0.0   \n",
       "4                0.00               1.00                 0.0   \n",
       "\n",
       "   overall_sentiment  word_count  \n",
       "0             0.0000           0  \n",
       "1             0.0000           0  \n",
       "2            -0.4003          10  \n",
       "3             0.0000          10  \n",
       "4             0.0000           6  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CSV_DATA_PATH = os.path.join(\"data\", \"mbti_2.csv\")\n",
    "\n",
    "df = load_csv_data(csv_file_path=CSV_DATA_PATH)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url_count</th>\n",
       "      <th>noun_freq</th>\n",
       "      <th>verb_freq</th>\n",
       "      <th>cardinal_digits_freq</th>\n",
       "      <th>adjective_freq</th>\n",
       "      <th>adverb_freq</th>\n",
       "      <th>preposition_freq</th>\n",
       "      <th>interjection_freq</th>\n",
       "      <th>determiner_freq</th>\n",
       "      <th>overall_sentiment</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.4003</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   url_count  noun_freq  verb_freq  cardinal_digits_freq  adjective_freq  \\\n",
       "0          1          0          0                     0               0   \n",
       "1          1          0          0                     0               0   \n",
       "2          2          5          1                     0               1   \n",
       "3          0          2          2                     0               1   \n",
       "4          2          2          0                     0               1   \n",
       "\n",
       "   adverb_freq  preposition_freq  interjection_freq  determiner_freq  \\\n",
       "0            0                 0                  0                0   \n",
       "1            0                 0                  0                0   \n",
       "2            1                 0                  0                0   \n",
       "3            1                 1                  0                2   \n",
       "4            0                 3                  0                0   \n",
       "\n",
       "   overall_sentiment  word_count  \n",
       "0             0.0000           0  \n",
       "1             0.0000           0  \n",
       "2            -0.4003          10  \n",
       "3             0.0000          10  \n",
       "4             0.0000           6  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = df[[\n",
    "    \"url_count\",\n",
    "    \"noun_freq\",\n",
    "    \"verb_freq\",\n",
    "    \"cardinal_digits_freq\",\n",
    "    \"adjective_freq\",\n",
    "    \"adverb_freq\",\n",
    "    \"preposition_freq\",\n",
    "    \"interjection_freq\",\n",
    "    \"determiner_freq\",\n",
    "    \"overall_sentiment\",\n",
    "    \"word_count\"\n",
    "]]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>e_vs_i</th>\n",
       "      <th>s_vs_n</th>\n",
       "      <th>t_vs_f</th>\n",
       "      <th>j_vs_p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   e_vs_i  s_vs_n  t_vs_f  j_vs_p\n",
       "0       1       0       0       0\n",
       "1       1       0       0       0\n",
       "2       1       0       0       0\n",
       "3       1       0       0       0\n",
       "4       1       0       0       0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "targets = df[[\"e_vs_i\", \"s_vs_n\", \"t_vs_f\", \"j_vs_p\"]].apply(label_encoder.fit_transform)\n",
    "targets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>e_vs_i</th>\n",
       "      <th>s_vs_n</th>\n",
       "      <th>t_vs_f</th>\n",
       "      <th>j_vs_p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>J</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>J</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>J</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>J</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>J</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  e_vs_i s_vs_n t_vs_f j_vs_p\n",
       "0      I      N      F      J\n",
       "1      I      N      F      J\n",
       "2      I      N      F      J\n",
       "3      I      N      F      J\n",
       "4      I      N      F      J"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[[\"e_vs_i\", \"s_vs_n\", \"t_vs_f\", \"j_vs_p\"]].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initial XGBoost Classifier for Extraversion vs Introversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "Extraversion       0.96      0.00      0.01     97582\n",
      "Introversion       0.77      1.00      0.87    325263\n",
      "\n",
      "    accuracy                           0.77    422845\n",
      "   macro avg       0.86      0.50      0.44    422845\n",
      "weighted avg       0.81      0.77      0.67    422845\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y = targets[\"e_vs_i\"].to_numpy()\n",
    "\n",
    "xgboost_classifier_e_vs_i = XGBClassifier(eval_metric=f1_score, random_state=42)\n",
    "xgboost_classifier_e_vs_i.fit(X, y)\n",
    "y_pred = xgboost_classifier_e_vs_i.predict(X)\n",
    "print(classification_report(y, y_pred, target_names=[\"Extraversion\", \"Introversion\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation CV Macro F1 Scores: [0.43553382 0.43598057 0.43551852 0.43556097 0.43554912]\n",
      "Mean Validation CV Macro F1 Score: 0.4356286006361202\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cv_scores = cross_val_score(xgboost_classifier_e_vs_i, X, y, scoring=\"f1_macro\")\n",
    "mean_cv_score = np.mean(cv_scores)\n",
    "print(f\"Validation CV Macro F1 Scores: {cv_scores}\")\n",
    "print(f\"Mean Validation CV Macro F1 Score: {mean_cv_score}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initial XGBoost Classifier for Intuition vs Sensing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Intuition       0.86      1.00      0.93    364822\n",
      "     Sensing       0.99      0.00      0.00     58023\n",
      "\n",
      "    accuracy                           0.86    422845\n",
      "   macro avg       0.93      0.50      0.47    422845\n",
      "weighted avg       0.88      0.86      0.80    422845\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "y = targets[\"s_vs_n\"].to_numpy()\n",
    "\n",
    "xgboost_classifier_s_vs_n = XGBClassifier(eval_metric=f1_score, random_state=42)\n",
    "xgboost_classifier_s_vs_n.fit(X, y)\n",
    "y_pred = xgboost_classifier_s_vs_n.predict(X)\n",
    "print(classification_report(y, y_pred, target_names=[\"Intuition\", \"Sensing\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation CV Macro F1 Scores: [0.46322481 0.46314941 0.46312555 0.4634097  0.46347464]\n",
      "Mean Validation CV Macro F1 Score: 0.4632768223870796\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cv_scores = cross_val_score(xgboost_classifier_s_vs_n, X, y, scoring=\"f1_macro\")\n",
    "mean_cv_score = np.mean(cv_scores)\n",
    "print(f\"Validation CV Macro F1 Scores: {cv_scores}\")\n",
    "print(f\"Mean Validation CV Macro F1 Score: {mean_cv_score}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initial XGBoost Classifier for Thinking vs Feeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Feeling       0.58      0.76      0.66    229312\n",
      "    Thinking       0.56      0.35      0.43    193533\n",
      "\n",
      "    accuracy                           0.58    422845\n",
      "   macro avg       0.57      0.56      0.55    422845\n",
      "weighted avg       0.57      0.58      0.56    422845\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "y = targets[\"t_vs_f\"].to_numpy()\n",
    "\n",
    "xgboost_classifier_t_vs_f = XGBClassifier(eval_metric=f1_score, random_state=42)\n",
    "xgboost_classifier_t_vs_f.fit(X, y)\n",
    "y_pred = xgboost_classifier_t_vs_f.predict(X)\n",
    "print(classification_report(y, y_pred, target_names=[\"Feeling\", \"Thinking\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation CV Macro F1 Scores: [0.52595853 0.52960918 0.52267723 0.52548048 0.52585923]\n",
      "Mean Validation CV Macro F1 Score: 0.5259169280081107\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cv_scores = cross_val_score(xgboost_classifier_t_vs_f, X, y, scoring=\"f1_macro\")\n",
    "mean_cv_score = np.mean(cv_scores)\n",
    "print(f\"Validation CV Macro F1 Scores: {cv_scores}\")\n",
    "print(f\"Mean Validation CV Macro F1 Score: {mean_cv_score}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initial XGBoost Classifier for Judging vs Percieving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Judging       0.84      0.02      0.03    167110\n",
      "  Percieving       0.61      1.00      0.76    255735\n",
      "\n",
      "    accuracy                           0.61    422845\n",
      "   macro avg       0.72      0.51      0.39    422845\n",
      "weighted avg       0.70      0.61      0.47    422845\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "y = targets[\"j_vs_p\"].to_numpy()\n",
    "\n",
    "xgboost_classifier_j_vs_p = XGBClassifier(eval_metric=f1_score, random_state=42)\n",
    "xgboost_classifier_j_vs_p.fit(X, y)\n",
    "y_pred = xgboost_classifier_j_vs_p.predict(X)\n",
    "print(classification_report(y, y_pred, target_names=[\"Judging\", \"Percieving\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation CV Macro F1 Scores: [0.38518657 0.38459218 0.3854913  0.38414562 0.38407729]\n",
      "Mean Validation CV Macro F1 Score: 0.38469859256001027\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cv_scores = cross_val_score(xgboost_classifier_j_vs_p, X, y, scoring=\"f1_macro\")\n",
    "mean_cv_score = np.mean(cv_scores)\n",
    "print(f\"Validation CV Macro F1 Scores: {cv_scores}\")\n",
    "print(f\"Mean Validation CV Macro F1 Score: {mean_cv_score}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMOTE Oversampling\n",
    "\n",
    "One of the observations from the initial classifiers fitted across the 4 axes of personality factors is that there tends to be low macro averaged F1 scores, which is caused by the class imbalances in the data. We focus here on macro averaged F1 scores since it weighs each class equally, which is helpful in situations where class imbalances are prevalent. \n",
    "\n",
    "One way to deal with this is to oversample the minority class before fitting the classifier so that the imbalance effect can be somewhat mitigated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def build_classifier_with_smote(model=XGBClassifier(eval_metric=f1_score, random_state=42), sampling_strategy=\"auto\"):\n",
    "    \"\"\" Return the given model, with intitial SMOTE over-sampling.\n",
    "\n",
    "    Args:\n",
    "    - model - The classfication model to fit onto input data.\n",
    "    - sampling_strategy - Sampling information to resample the data set. See API reference for imbalance-learn's SMOTE() for more details.\n",
    "\n",
    "    Returns:\n",
    "    - pipeline - A pipeline that oversamples fitted data using SMOTE and then proceeds to fit the input model onto the data being fed into the pipeline.\n",
    "    \"\"\"\n",
    "    over_sampler = SMOTE(sampling_strategy=sampling_strategy, random_state=42)\n",
    "    pipeline = Pipeline([\n",
    "        (\"over_sampler\", over_sampler),\n",
    "        (\"model\", model)\n",
    "    ])\n",
    "\n",
    "    return pipeline    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### XGBoost Classifier for Extraversion vs Introversion using SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "Extraversion       0.27      0.35      0.30     97582\n",
      "Introversion       0.78      0.71      0.75    325263\n",
      "\n",
      "    accuracy                           0.63    422845\n",
      "   macro avg       0.53      0.53      0.52    422845\n",
      "weighted avg       0.67      0.63      0.64    422845\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y = targets[\"e_vs_i\"].values\n",
    "\n",
    "xgboost_classifier_e_vs_i = build_classifier_with_smote()\n",
    "xgboost_classifier_e_vs_i.fit(X, y)\n",
    "y_pred = xgboost_classifier_e_vs_i.predict(X)\n",
    "print(classification_report(y, y_pred, target_names=[\"Extraversion\", \"Introversion\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation CV Macro F1 Scores: [0.5017726  0.50625973 0.50626898 0.5085948  0.5035952 ]\n",
      "Mean Validation CV Macro F1 Score: 0.5052982618895431\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cv_scores = cross_val_score(xgboost_classifier_e_vs_i, X, y, scoring=\"f1_macro\")\n",
    "mean_cv_score = np.mean(cv_scores)\n",
    "print(f\"Validation CV Macro F1 Scores: {cv_scores}\")\n",
    "print(f\"Mean Validation CV Macro F1 Score: {mean_cv_score}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### XGBoost Classifier for Intuition vs Sensing using SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Intuition       0.87      0.70      0.77    364822\n",
      "     Sensing       0.16      0.35      0.22     58023\n",
      "\n",
      "    accuracy                           0.65    422845\n",
      "   macro avg       0.51      0.53      0.50    422845\n",
      "weighted avg       0.77      0.65      0.70    422845\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y = targets[\"s_vs_n\"].values\n",
    "\n",
    "xgboost_classifier_s_vs_n = build_classifier_with_smote()\n",
    "xgboost_classifier_s_vs_n.fit(X, y)\n",
    "y_pred = xgboost_classifier_s_vs_n.predict(X)\n",
    "print(classification_report(y, y_pred, target_names=[\"Intuition\", \"Sensing\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation CV Macro F1 Scores: [0.48286674 0.48030735 0.48182886 0.48182858 0.48090159]\n",
      "Mean Validation CV Macro F1 Score: 0.4815466247681718\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cv_scores = cross_val_score(xgboost_classifier_s_vs_n, X, y, scoring=\"f1_macro\")\n",
    "mean_cv_score = np.mean(cv_scores)\n",
    "print(f\"Validation CV Macro F1 Scores: {cv_scores}\")\n",
    "print(f\"Mean Validation CV Macro F1 Score: {mean_cv_score}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### XGBoost Classifier for Thinking vs Feeling using SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Feeling       0.61      0.55      0.58    229312\n",
      "    Thinking       0.52      0.59      0.55    193533\n",
      "\n",
      "    accuracy                           0.57    422845\n",
      "   macro avg       0.57      0.57      0.57    422845\n",
      "weighted avg       0.57      0.57      0.57    422845\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y = targets[\"t_vs_f\"].values\n",
    "\n",
    "xgboost_classifier_t_vs_f = build_classifier_with_smote()\n",
    "xgboost_classifier_t_vs_f.fit(X, y)\n",
    "y_pred = xgboost_classifier_t_vs_f.predict(X)\n",
    "print(classification_report(y, y_pred, target_names=[\"Feeling\", \"Thinking\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation CV Macro F1 Scores: [0.54294513 0.54372997 0.54317304 0.54150478 0.54114781]\n",
      "Mean Validation CV Macro F1 Score: 0.5425001485950848\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cv_scores = cross_val_score(xgboost_classifier_t_vs_f, X, y, scoring=\"f1_macro\")\n",
    "mean_cv_score = np.mean(cv_scores)\n",
    "print(f\"Validation CV Macro F1 Scores: {cv_scores}\")\n",
    "print(f\"Mean Validation CV Macro F1 Score: {mean_cv_score}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### XGBoost Classifier for Judging vs Perceiving using SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Judging       0.44      0.40      0.42    167110\n",
      "  Percieving       0.63      0.67      0.65    255735\n",
      "\n",
      "    accuracy                           0.56    422845\n",
      "   macro avg       0.54      0.53      0.53    422845\n",
      "weighted avg       0.56      0.56      0.56    422845\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y = targets[\"j_vs_p\"].values\n",
    "\n",
    "xgboost_classifier_j_vs_p = build_classifier_with_smote()\n",
    "xgboost_classifier_j_vs_p.fit(X, y)\n",
    "y_pred = xgboost_classifier_j_vs_p.predict(X)\n",
    "print(classification_report(y, y_pred, target_names=[\"Judging\", \"Percieving\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation CV Macro F1 Scores: [0.50344595 0.50521136 0.50303166 0.50099618 0.50374662]\n",
      "Mean Validation CV Macro F1 Score: 0.5032863537090837\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cv_scores = cross_val_score(xgboost_classifier_j_vs_p, X, y)\n",
    "mean_cv_score = np.mean(cv_scores)\n",
    "print(f\"Validation CV Macro F1 Scores: {cv_scores}\")\n",
    "print(f\"Mean Validation CV Macro F1 Score: {mean_cv_score}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From performing some over-sampling of the data points within the minority classes, we can see some improvements to the F1 scores of the minority classes. We can also see some slight improvements to the macro averaged F1 scores for the classfiers. The authors of the original SMOTE research paper suggest combining soem undersampling techiques along side SMOTE for better performance, and so this will be tried out in the next section."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SMOTE + Edited Nearest Neighbors Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def build_classifier_with_smote_enn(model=XGBClassifier(eval_metric=f1_score, random_state=42), sampling_strategy=\"auto\"):\n",
    "    \"\"\" Return the given model, with intitial SMOTE over-sampling.\n",
    "\n",
    "    Args:\n",
    "    - model - The classfication model to fit onto input data.\n",
    "    - sampling_strategy - Sampling information to resample the data set. See API reference for imbalance-learn's SMOTE() for more details.\n",
    "\n",
    "    Returns:\n",
    "    - pipeline - A pipeline that oversamples fitted data using SMOTE and then proceeds to fit the input model onto the data being fed into the pipeline.\n",
    "    \"\"\"\n",
    "    resampler = SMOTEENN(sampling_strategy=sampling_strategy, random_state=42)\n",
    "    pipeline = Pipeline([\n",
    "        (\"resampler\", resampler),\n",
    "        (\"model\", model)\n",
    "    ])\n",
    "\n",
    "    return pipeline    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extraversion vs Introversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "Extraversion       0.25      0.68      0.36     97582\n",
      "Introversion       0.80      0.38      0.52    325263\n",
      "\n",
      "    accuracy                           0.45    422845\n",
      "   macro avg       0.52      0.53      0.44    422845\n",
      "weighted avg       0.67      0.45      0.48    422845\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y = targets[\"e_vs_i\"].values\n",
    "\n",
    "xgboost_classifier_e_vs_i = build_classifier_with_smote_enn()\n",
    "xgboost_classifier_e_vs_i.fit(X, y)\n",
    "y_pred = xgboost_classifier_e_vs_i.predict(X)\n",
    "print(classification_report(y, y_pred, target_names=[\"Extraversion\", \"Introversion\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Intuition vs Sensing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Intuition       0.87      0.53      0.66    364822\n",
      "     Sensing       0.15      0.51      0.23     58023\n",
      "\n",
      "    accuracy                           0.53    422845\n",
      "   macro avg       0.51      0.52      0.45    422845\n",
      "weighted avg       0.77      0.53      0.60    422845\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y = targets[\"s_vs_n\"].values\n",
    "\n",
    "xgboost_classifier_s_vs_n = build_classifier_with_smote_enn()\n",
    "xgboost_classifier_s_vs_n.fit(X, y)\n",
    "y_pred = xgboost_classifier_s_vs_n.predict(X)\n",
    "print(classification_report(y, y_pred, target_names=[\"Intuition\", \"Sensing\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feeling vs Thinking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Feeling       0.61      0.41      0.49    229312\n",
      "    Thinking       0.50      0.69      0.58    193533\n",
      "\n",
      "    accuracy                           0.54    422845\n",
      "   macro avg       0.56      0.55      0.54    422845\n",
      "weighted avg       0.56      0.54      0.53    422845\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y = targets[\"t_vs_f\"].values\n",
    "\n",
    "xgboost_classifier_t_vs_f = build_classifier_with_smote_enn()\n",
    "xgboost_classifier_t_vs_f.fit(X, y)\n",
    "y_pred = xgboost_classifier_t_vs_f.predict(X)\n",
    "print(classification_report(y, y_pred, target_names=[\"Feeling\", \"Thinking\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Judging vs Percieving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Judging       0.41      0.75      0.53    167110\n",
      "  Percieving       0.65      0.30      0.41    255735\n",
      "\n",
      "    accuracy                           0.48    422845\n",
      "   macro avg       0.53      0.52      0.47    422845\n",
      "weighted avg       0.55      0.48      0.46    422845\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y = targets[\"j_vs_p\"].values\n",
    "\n",
    "xgboost_classifier_j_vs_p = build_classifier_with_smote_enn()\n",
    "xgboost_classifier_j_vs_p.fit(X, y)\n",
    "y_pred = xgboost_classifier_j_vs_p.predict(X)\n",
    "print(classification_report(y, y_pred, target_names=[\"Judging\", \"Percieving\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the above results that while there is improvement in the F1 score of the minority classes, the majority classes F1 and the macro averaged F1 score were found to have gotten worse. Next, we shall try another method of SMOTE oversampling combined with undersampling."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SMOTE + Tomek Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def build_classifier_with_smote_tomek(model=XGBClassifier(eval_metric=f1_score, random_state=42), sampling_strategy=\"auto\"):\n",
    "    \"\"\" Return the given model, with intitial SMOTE over-sampling.\n",
    "\n",
    "    Args:\n",
    "    - model - The classfication model to fit onto input data.\n",
    "    - sampling_strategy - Sampling information to resample the data set. See API reference for imbalance-learn's SMOTE() for more details.\n",
    "\n",
    "    Returns:\n",
    "    - pipeline - A pipeline that oversamples fitted data using SMOTE and then proceeds to fit the input model onto the data being fed into the pipeline.\n",
    "    \"\"\"\n",
    "    resampler = SMOTETomek(sampling_strategy=sampling_strategy, random_state=42)\n",
    "    pipeline = Pipeline([\n",
    "        (\"resampler\", resampler),\n",
    "        (\"model\", model)\n",
    "    ])\n",
    "\n",
    "    return pipeline    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extraversion vs Introversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "Extraversion       0.27      0.34      0.30     97582\n",
      "Introversion       0.78      0.72      0.75    325263\n",
      "\n",
      "    accuracy                           0.63    422845\n",
      "   macro avg       0.53      0.53      0.52    422845\n",
      "weighted avg       0.66      0.63      0.65    422845\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y = targets[\"e_vs_i\"].values\n",
    "\n",
    "xgboost_classifier_e_vs_i = build_classifier_with_smote_tomek()\n",
    "xgboost_classifier_e_vs_i.fit(X, y)\n",
    "y_pred = xgboost_classifier_e_vs_i.predict(X)\n",
    "print(classification_report(y, y_pred, target_names=[\"Extraversion\", \"Introversion\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Intuition vs Sensing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Intuition       0.87      0.70      0.78    364822\n",
      "     Sensing       0.16      0.35      0.22     58023\n",
      "\n",
      "    accuracy                           0.65    422845\n",
      "   macro avg       0.51      0.52      0.50    422845\n",
      "weighted avg       0.77      0.65      0.70    422845\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y = targets[\"s_vs_n\"].values\n",
    "\n",
    "xgboost_classifier_s_vs_n = build_classifier_with_smote_tomek()\n",
    "xgboost_classifier_s_vs_n.fit(X, y)\n",
    "y_pred = xgboost_classifier_s_vs_n.predict(X)\n",
    "print(classification_report(y, y_pred, target_names=[\"Intuition\", \"Sensing\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feeling vs Thinking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Feeling       0.61      0.56      0.58    229312\n",
      "    Thinking       0.52      0.58      0.55    193533\n",
      "\n",
      "    accuracy                           0.57    422845\n",
      "   macro avg       0.57      0.57      0.57    422845\n",
      "weighted avg       0.57      0.57      0.57    422845\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y = targets[\"t_vs_f\"].values\n",
    "\n",
    "xgboost_classifier_t_vs_f = build_classifier_with_smote_tomek()\n",
    "xgboost_classifier_t_vs_f.fit(X, y)\n",
    "y_pred = xgboost_classifier_t_vs_f.predict(X)\n",
    "print(classification_report(y, y_pred, target_names=[\"Feeling\", \"Thinking\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Judging vs Percieving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Judging       0.44      0.40      0.42    167110\n",
      "  Percieving       0.63      0.67      0.65    255735\n",
      "\n",
      "    accuracy                           0.56    422845\n",
      "   macro avg       0.53      0.53      0.53    422845\n",
      "weighted avg       0.55      0.56      0.56    422845\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y = targets[\"j_vs_p\"].values\n",
    "\n",
    "xgboost_classifier_j_vs_p = build_classifier_with_smote_tomek()\n",
    "xgboost_classifier_j_vs_p.fit(X, y)\n",
    "y_pred = xgboost_classifier_j_vs_p.predict(X)\n",
    "print(classification_report(y, y_pred, target_names=[\"Judging\", \"Percieving\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results obtained via using the default settings of Tomek Links along side SMOTE does not seem to improve model performance in terms of F1 scores of the classes and macro averaged F1 scores any more than just using SMOTE."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_myers_briggs_personalities",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8c5e62d055c9f5019009eeac34d5848eef589b72b0bbb976c58401fa648b29a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
