{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Development\n",
    "\n",
    "Based on the previous work in the other notebooks, we managed to gain some initial insight for what might be contributing features when it comes to determining an online users MBTI personality factors based on what they posted, such as the overall sentiment of their posts, length of their posts, noun and verb frequency of their posts, etc. Here we try to develop a model with more emphasis on performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE + Word2Vec + Logistic Regression\n",
    "\n",
    "In the paper *Ryan, G.; Katarina, P.; Suhartono, D. MBTI Personality Prediction Using Machine Learning and SMOTE for Balancing Data Based on Statement Sentences. Information 2023, 14, 217*, a combination of SMOTE oversampling, Word2Vec word embeddings and Logistic Regression was used to achieve an average F1 - Score of 0.8337 across the four dimensions of MBTI personalities, namely:\n",
    "- Extraversion vs Introversion\n",
    "- Sensing vs Intuition\n",
    "- Thinking vs Feeling\n",
    "- Judgment vs Perception\n",
    "\n",
    "In this section we will attempt to implement this model from the paper using their methodology."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_DATA_PATH = os.path.join(\"data\", \"mbti_2.csv\")\n",
    "\n",
    "def load_csv_data(csv_file_path: str):\n",
    "    \"\"\" Load data from a given csv file into a pandas DataFrame object.\n",
    "\n",
    "    Args:\n",
    "    - csv_file_path (str) - The file path of the csv file containing the desired data to load into a pandas DataFrame object.\n",
    "\n",
    "    Returns:\n",
    "    - data (pandas.DataFrame) - The loaded data as a pandas DataFrame object.\n",
    "    \"\"\"\n",
    "    assert csv_file_path.endswith(\".csv\")\n",
    "\n",
    "    return pd.read_csv(csv_file_path)\n",
    "\n",
    "def lemmatize_words(text: str):\n",
    "    \"\"\" Lemmatize the input string of text.\n",
    "\n",
    "    Args:\n",
    "    - text (str) - The input string of text to lemmatize.\n",
    "\n",
    "    Returns:\n",
    "    - lemmatized (str) - The lemmatized version of the string of text that was given to the function.\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    words = text.split()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    lemmatized = \" \".join(words)\n",
    "\n",
    "    return lemmatized\n",
    "\n",
    "def preprocess_df(df: pd.DataFrame):\n",
    "    \"\"\" Helper function for preprocessing the input pandas dataframe by helping clean up input user posts.\n",
    "\n",
    "    Args:\n",
    "    - df (pd.DataFrame) - The input dataframe object containing the user posts to clean up.\n",
    "\n",
    "    Returns:\n",
    "    - df (pd.DataFrame) - The given dataframe object after cleaning has been applied to the user posts data.\n",
    "    \"\"\"   \n",
    "\n",
    "    # Keep the End Of Sentence characters\n",
    "    df[\"posts_no_url\"] = df[\"posts_no_url\"].apply(lambda x: re.sub(r'\\.', ' EOSTokenDot ', str(x) + \" \"))\n",
    "    df[\"posts_no_url\"] = df[\"posts_no_url\"].apply(lambda x: re.sub(r'\\?', ' EOSTokenQuest ', str(x) + \" \"))\n",
    "    df[\"posts_no_url\"] = df[\"posts_no_url\"].apply(lambda x: re.sub(r'!', ' EOSTokenExs ', str(x) + \" \"))\n",
    "    \n",
    "    # Strip Punctation\n",
    "    df[\"posts_no_url\"] = df[\"posts_no_url\"].apply(lambda x: re.sub(r'[\\.+]', \".\", str(x)))\n",
    "\n",
    "    # Remove multiple fullstops\n",
    "    # df[\"posts_no_url\"] = df[\"posts_no_url\"].apply(lambda x: re.sub(r'[^\\w\\s]','', str(x)))\n",
    "\n",
    "    # Remove Non-words\n",
    "    # df[\"posts_no_url\"] = df[\"posts_no_url\"].apply(lambda x: re.sub(r'[^a-zA-Z\\s]','', str(x)))\n",
    "\n",
    "    # Convert posts to lowercase\n",
    "    df[\"posts_no_url\"] = df[\"posts_no_url\"].apply(lambda x: str(x).lower())\n",
    "\n",
    "    # Remove multiple letter repeating words\n",
    "    # df[\"posts_no_url\"] = df[\"posts_no_url\"].apply(lambda x: re.sub(r'([a-z])\\1{2,}[\\s|\\w]*','', str(x)))\n",
    "    \n",
    "    # Strip trailing whitespaces\n",
    "    df[\"posts_no_url\"] = df[\"posts_no_url\"].apply(lambda x: str(x).strip())\n",
    "\n",
    "    # Remove rows with no text\n",
    "    df[\"posts_no_url\"] = df[\"posts_no_url\"].replace('', np.nan)\n",
    "    df[\"posts_no_url\"] = df[\"posts_no_url\"].replace('nan', np.nan)\n",
    "    df[\"posts_no_url\"] = df[\"posts_no_url\"].replace(\"'\", np.nan)\n",
    "    df[\"posts_no_url\"] = df[\"posts_no_url\"].replace(\"''\", np.nan)\n",
    "    df[\"posts_no_url\"] = df[\"posts_no_url\"].replace('\"', np.nan)\n",
    "    df[\"posts_no_url\"] = df[\"posts_no_url\"].replace('\"\"', np.nan)\n",
    "    df.dropna(subset=[\"posts_no_url\"], inplace=True)\n",
    "\n",
    "    # Lemmatize posts\n",
    "    df[\"posts_no_url\"] = df[\"posts_no_url\"].apply(lemmatize_words)\n",
    "\n",
    "    # Tokenize posts\n",
    "    df[\"posts_no_url_tokens\"] = df[\"posts_no_url\"].apply(wordpunct_tokenize)\n",
    "\n",
    "    return df[[\"e_vs_i\", \"s_vs_n\", \"t_vs_f\", \"j_vs_p\", \"type\", \"posts\", \"posts_no_url\", \"posts_no_url_tokens\"]]\n",
    "\n",
    "def binarize_targets(df: pd.DataFrame):\n",
    "    \"\"\" Apply 0/1 labels to the input classes in the df.\n",
    "\n",
    "    Args:\n",
    "    - df (pd.DataFrame) - The input dataframe object containing the classes to numerically label.\n",
    "\n",
    "    Returns:\n",
    "    - df (pd.DataFrame) - The given dataframe object after numerically labelling classes.\n",
    "    \"\"\"\n",
    "    binary_map = {\n",
    "        'I': 0, \n",
    "        'E': 1, \n",
    "        'N': 0, \n",
    "        'S': 1, \n",
    "        'F': 0, \n",
    "        'T': 1, \n",
    "        'J': 0, \n",
    "        'P': 1\n",
    "    }\n",
    "\n",
    "    df[\"EI\"] = df[\"e_vs_i\"].apply(lambda x: binary_map[str(x)])\n",
    "    df[\"SN\"] = df[\"s_vs_n\"].apply(lambda x: binary_map[str(x)])\n",
    "    df[\"TF\"] = df[\"t_vs_f\"].apply(lambda x: binary_map[str(x)])\n",
    "    df[\"JP\"] = df[\"j_vs_p\"].apply(lambda x: binary_map[str(x)])\n",
    "\n",
    "    df[\"target_vec\"] = df.apply(lambda x: [\n",
    "        x[\"EI\"],\n",
    "        x[\"SN\"],\n",
    "        x[\"TF\"],\n",
    "        x[\"JP\"]\n",
    "    ], axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def preprocess_dataset(csv_file_path: str = CSV_DATA_PATH):\n",
    "    \"\"\" Load and preprocess data to be used for model development.\n",
    "\n",
    "    Args:\n",
    "    - csv_file_path (str) - The file path of the csv file containing the desired data to load into a pandas DataFrame object.\n",
    "\n",
    "    Returns:\n",
    "    - numpy arrays of training and test split data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Apply preprocessing to input user posts\n",
    "    df = load_csv_data(csv_file_path)\n",
    "    df = preprocess_df(df)\n",
    "\n",
    "    # Preprocess target labels into a binary vector\n",
    "    df = binarize_targets(df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>e_vs_i</th>\n",
       "      <th>s_vs_n</th>\n",
       "      <th>t_vs_f</th>\n",
       "      <th>j_vs_p</th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "      <th>posts_no_url</th>\n",
       "      <th>posts_no_url_tokens</th>\n",
       "      <th>EI</th>\n",
       "      <th>SN</th>\n",
       "      <th>TF</th>\n",
       "      <th>JP</th>\n",
       "      <th>target_vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>J</td>\n",
       "      <td>INFJ</td>\n",
       "      <td>enfp and intj moments  https://www.youtube.com...</td>\n",
       "      <td>enfp and intj moment sportscenter not top ten ...</td>\n",
       "      <td>[enfp, and, intj, moment, sportscenter, not, t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>J</td>\n",
       "      <td>INFJ</td>\n",
       "      <td>What has been the most life-changing experienc...</td>\n",
       "      <td>what ha been the most life-changing experience...</td>\n",
       "      <td>[what, ha, been, the, most, life, -, changing,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>J</td>\n",
       "      <td>INFJ</td>\n",
       "      <td>http://www.youtube.com/watch?v=vXZeYwwRDw8   h...</td>\n",
       "      <td>on repeat for most of today eostokendot</td>\n",
       "      <td>[on, repeat, for, most, of, today, eostokendot]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>J</td>\n",
       "      <td>INFJ</td>\n",
       "      <td>May the PerC Experience immerse you.</td>\n",
       "      <td>may the perc experience immerse you eostokendot</td>\n",
       "      <td>[may, the, perc, experience, immerse, you, eos...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>J</td>\n",
       "      <td>INFJ</td>\n",
       "      <td>The last thing my INFJ friend posted on his fa...</td>\n",
       "      <td>the last thing my infj friend posted on his fa...</td>\n",
       "      <td>[the, last, thing, my, infj, friend, posted, o...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  e_vs_i s_vs_n t_vs_f j_vs_p  type  \\\n",
       "2      I      N      F      J  INFJ   \n",
       "3      I      N      F      J  INFJ   \n",
       "4      I      N      F      J  INFJ   \n",
       "5      I      N      F      J  INFJ   \n",
       "6      I      N      F      J  INFJ   \n",
       "\n",
       "                                               posts  \\\n",
       "2  enfp and intj moments  https://www.youtube.com...   \n",
       "3  What has been the most life-changing experienc...   \n",
       "4  http://www.youtube.com/watch?v=vXZeYwwRDw8   h...   \n",
       "5               May the PerC Experience immerse you.   \n",
       "6  The last thing my INFJ friend posted on his fa...   \n",
       "\n",
       "                                        posts_no_url  \\\n",
       "2  enfp and intj moment sportscenter not top ten ...   \n",
       "3  what ha been the most life-changing experience...   \n",
       "4            on repeat for most of today eostokendot   \n",
       "5    may the perc experience immerse you eostokendot   \n",
       "6  the last thing my infj friend posted on his fa...   \n",
       "\n",
       "                                 posts_no_url_tokens  EI  SN  TF  JP  \\\n",
       "2  [enfp, and, intj, moment, sportscenter, not, t...   0   0   0   0   \n",
       "3  [what, ha, been, the, most, life, -, changing,...   0   0   0   0   \n",
       "4    [on, repeat, for, most, of, today, eostokendot]   0   0   0   0   \n",
       "5  [may, the, perc, experience, immerse, you, eos...   0   0   0   0   \n",
       "6  [the, last, thing, my, infj, friend, posted, o...   0   0   0   0   \n",
       "\n",
       "     target_vec  \n",
       "2  [0, 0, 0, 0]  \n",
       "3  [0, 0, 0, 0]  \n",
       "4  [0, 0, 0, 0]  \n",
       "5  [0, 0, 0, 0]  \n",
       "6  [0, 0, 0, 0]  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = preprocess_dataset()\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following preprocessing steps were applied to the dataset:\n",
    "- Converting letters to lowercase\n",
    "- Removing links\n",
    "- Removing punctuations\n",
    "- Removing stopwords\n",
    "\n",
    "Lematization was also applied after the above steps have been conducted, and the resulting text for each post tokenized."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word Embeddings and Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Word2Vec model\n",
    "w2w_model = Word2Vec(sentences=df[\"posts_no_url_tokens\"], vector_size=100, window=5, min_count=5, epochs=50)\n",
    "\n",
    "def get_train_test_split(training_fraction: float, df: pd.DataFrame, mbti_dim: str):\n",
    "    \"\"\" Get a training and test dataset split.\n",
    "\n",
    "    Args:\n",
    "    - training_proportion (float) - The fraction of the dataset to be used as training data.\n",
    "    - df (pd.DataFrame) - The pandas dataframe object containing the data to be split into train and test sets.\n",
    "    - mbti_dim (str) - The column name of the target MBTI personality dimension.\n",
    "\n",
    "    Returns:\n",
    "    - Train and test datasets where the target variable is the desired MBTI personality dimension (mbti_dim).\n",
    "    \"\"\"\n",
    "\n",
    "    # Split into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df[\"posts_no_url_tokens\"], df[mbti_dim], test_size=(1 - training_fraction), random_state=42)\n",
    "\n",
    "    X_train = np.array([vectorize(post) for post in X_train])\n",
    "    X_test = np.array([vectorize(post) for post in X_test])\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def vectorize(post: list):\n",
    "    \"\"\" Vectorize a sentence to a vector representation using Word2Vec CBOW.\n",
    "\n",
    "    Args:\n",
    "    - post (str) - A post to vectorize. Here, the post is passed into the function as a list of word tokens.\n",
    "\n",
    "    Returns:\n",
    "    - Word2Vec word embedding (CBOW).\n",
    "    \"\"\"\n",
    "\n",
    "    # Vectorize sentence\n",
    "    words_vector = [w2w_model.wv[token] for token in post if token in w2w_model.wv]\n",
    "\n",
    "    if len(words_vector) == 0:\n",
    "        return np.zeros(100)\n",
    "    \n",
    "    words_vector = np.array(words_vector)\n",
    "    return words_vector.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.38337144  0.45117185  0.84649754 ...  0.36549509 -0.44799906\n",
      "   0.33048698]\n",
      " [-0.65333349  0.02470458 -0.22094555 ...  0.27971393 -0.38791531\n",
      "   0.0902233 ]\n",
      " [-0.30711505  0.16958897 -0.77727389 ...  1.09113503 -0.09072065\n",
      "  -0.20716502]\n",
      " ...\n",
      " [-0.72033012  0.27641124 -0.2661525  ... -0.90939134  0.32613787\n",
      "  -0.23198929]\n",
      " [ 0.94679403 -1.34951031 -1.78497899 ...  1.55978203  3.53002429\n",
      "  -0.48310328]\n",
      " [-0.01861813  0.86415511  1.20786262 ... -0.46497735 -0.22456919\n",
      "   0.24400607]]\n"
     ]
    }
   ],
   "source": [
    "X_train_EI, X_test_EI, y_train_EI, y_test_EI = get_train_test_split(0.7, df, \"EI\") \n",
    "print(X_train_EI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.04483092  0.96792394  0.09036332 -0.24387008  0.08042677  0.47072533\n",
      "  0.03963196  0.2330282  -0.57037592  0.14603905  0.8789649  -0.45852929\n",
      " -0.67614126 -0.39700082  0.13280533 -0.46391824  0.25712341 -0.32038507\n",
      " -0.00278139 -0.3006697  -0.00317743 -1.67118871  1.25529742  0.62555629\n",
      " -1.04633796  1.04782712 -0.26789513 -0.17357592  0.9648236  -0.30632395\n",
      " -0.17826284 -0.17365976 -0.31818551  1.21937883  0.53019416  0.3174592\n",
      "  1.21626151  0.49607623 -0.06919578  0.67353165 -0.82898343  0.72627795\n",
      " -0.89021772  0.14418924 -1.14582872 -0.30632871 -0.85016423  0.52901191\n",
      " -0.30181804  0.39766544 -0.12399467 -0.68355817 -0.26619494 -0.26260328\n",
      " -0.96291333  0.14073643  0.68730819  0.33279404  0.37593246  0.95127153\n",
      " -0.11948744 -0.30713925  0.11967899 -0.21103275 -0.20507036  0.06893882\n",
      " -0.17073165  0.42157441 -0.23363309 -0.00808041 -0.95308846  0.18521222\n",
      "  0.90139633 -0.18391964 -0.33933204 -0.10248569 -0.3427673   0.7128638\n",
      "  0.39397308 -0.33363175 -0.74769706  1.05903888  0.05534673 -0.05909207\n",
      "  0.13483381  0.74300158  0.18073997  0.41047505  0.72626686  0.1264165\n",
      "  0.47164446  0.86535364  0.11804251  0.20725927  0.93744069 -0.27926031\n",
      "  1.6267873  -0.2468266   0.30128604  0.43897367]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_EI[160])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we split out user posts dataset into train and test sets, and vectorized the text data to be used as model inputs as Word2Vec word embeddings. The benefit with using word embeddings is that we can represent words in a numerical format (ML/DL models can only take in numerical input) that allows words with similar meanings to have the same representation, i.e. smilarity of meaning between words is captured in the word embeddings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_myers_briggs_personalities",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
