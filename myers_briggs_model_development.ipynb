{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Development\n",
    "\n",
    "Based on the previous work in the other notebooks, we managed to gain some initial insight for what might be contributing features when it comes to determining an online users MBTI personality factors based on what they posted, such as the overall sentiment of their posts, length of their posts, noun and verb frequency of their posts, etc. Here we try to develop a model with more emphasis on performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE + Word2Vec + Logistic Regression\n",
    "\n",
    "In the paper *Ryan, G.; Katarina, P.; Suhartono, D. MBTI Personality Prediction Using Machine Learning and SMOTE for Balancing Data Based on Statement Sentences. Information 2023, 14, 217*, a combination of SMOTE oversampling, Word2Vec word embeddings and Logistic Regression was used to achieve an average F1 - Score of 0.8337 across the four dimensions of MBTI personalities, namely:\n",
    "- Extraversion vs Introversion\n",
    "- Sensing vs Intuition\n",
    "- Thinking vs Feeling\n",
    "- Judgment vs Perception\n",
    "\n",
    "In this section we will attempt to implement this model from the paper using their methodology."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_DATA_PATH = os.path.join(\"data\", \"mbti_2.csv\")\n",
    "\n",
    "def load_csv_data(csv_file_path):\n",
    "    \"\"\" Load data from a given csv file into a pandas DataFrame object.\n",
    "\n",
    "    Args:\n",
    "    - csv_file_path (str) - The file path of the csv file containing the desired data to load into a pandas DataFrame object.\n",
    "\n",
    "    Returns:\n",
    "    - data (pandas.DataFrame) - The loaded data as a pandas DataFrame object.\n",
    "    \"\"\"\n",
    "    assert csv_file_path.endswith(\".csv\")\n",
    "\n",
    "    return pd.read_csv(csv_file_path)\n",
    "\n",
    "def lemmatize_words(text):\n",
    "    \"\"\" Lemmatize the input string of text.\n",
    "\n",
    "    Args:\n",
    "    - text (str) - The input string of text to lemmatize.\n",
    "\n",
    "    Returns:\n",
    "    - lemmatized (str) - The lemmatized version of the string of text that was given to the function.\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    words = text.split()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    lemmatized = \" \".join(words)\n",
    "\n",
    "    return lemmatized\n",
    "\n",
    "def preprocess_df(df):\n",
    "    \"\"\" Helper function for preprocessing the input pandas dataframe by helping clean up input user posts.\n",
    "\n",
    "    Args:\n",
    "    - df (pd.DataFrame) - The input dataframe object containing the user posts to clean up.\n",
    "\n",
    "    Returns:\n",
    "    - df (pd.DataFrame) - The given dataframe object after cleaning has been applied to the user posts data.\n",
    "    \"\"\"   \n",
    "\n",
    "    # Keep the End Of Sentence characters\n",
    "    df[\"posts_no_url\"] = df[\"posts_no_url\"].apply(lambda x: re.sub(r'\\.', ' EOSTokenDot ', str(x) + \" \"))\n",
    "    df[\"posts_no_url\"] = df[\"posts_no_url\"].apply(lambda x: re.sub(r'\\?', ' EOSTokenQuest ', str(x) + \" \"))\n",
    "    df[\"posts_no_url\"] = df[\"posts_no_url\"].apply(lambda x: re.sub(r'!', ' EOSTokenExs ', str(x) + \" \"))\n",
    "    \n",
    "    # Strip Punctation\n",
    "    df[\"posts_no_url\"] = df[\"posts_no_url\"].apply(lambda x: re.sub(r'[\\.+]', \".\", str(x)))\n",
    "\n",
    "    # Remove multiple fullstops\n",
    "    # df[\"posts_no_url\"] = df[\"posts_no_url\"].apply(lambda x: re.sub(r'[^\\w\\s]','', str(x)))\n",
    "\n",
    "    # Remove Non-words\n",
    "    # df[\"posts_no_url\"] = df[\"posts_no_url\"].apply(lambda x: re.sub(r'[^a-zA-Z\\s]','', str(x)))\n",
    "\n",
    "    # Convert posts to lowercase\n",
    "    df[\"posts_no_url\"] = df[\"posts_no_url\"].apply(lambda x: str(x).lower())\n",
    "\n",
    "    # Remove multiple letter repeating words\n",
    "    # df[\"posts_no_url\"] = df[\"posts_no_url\"].apply(lambda x: re.sub(r'([a-z])\\1{2,}[\\s|\\w]*','', str(x)))\n",
    "    \n",
    "    # Strip trailing whitespaces\n",
    "    df[\"posts_no_url\"] = df[\"posts_no_url\"].apply(lambda x: str(x).strip())\n",
    "\n",
    "    # Remove rows with no text\n",
    "    df[\"posts_no_url\"] = df[\"posts_no_url\"].replace('', np.nan)\n",
    "    df[\"posts_no_url\"] = df[\"posts_no_url\"].replace('nan', np.nan)\n",
    "    df[\"posts_no_url\"] = df[\"posts_no_url\"].replace(\"'\", np.nan)\n",
    "    df[\"posts_no_url\"] = df[\"posts_no_url\"].replace(\"''\", np.nan)\n",
    "    df[\"posts_no_url\"] = df[\"posts_no_url\"].replace('\"', np.nan)\n",
    "    df[\"posts_no_url\"] = df[\"posts_no_url\"].replace('\"\"', np.nan)\n",
    "    df.dropna(subset=[\"posts_no_url\"], inplace=True)\n",
    "\n",
    "    # Lemmatize posts\n",
    "    df[\"posts_no_url\"] = df[\"posts_no_url\"].apply(lemmatize_words)\n",
    "\n",
    "    # Generate word tokens\n",
    "    df[\"posts_no_url_tokens\"] = df[\"posts_no_url\"].apply(word_tokenize)\n",
    "\n",
    "    return df[[\"e_vs_i\", \"s_vs_n\", \"t_vs_f\", \"j_vs_p\", \"type\", \"posts\", \"posts_no_url\", \"posts_no_url_tokens\"]]\n",
    "\n",
    "def binarize_targets(df):\n",
    "    \"\"\" Apply 0/1 labels to the input classes in the df.\n",
    "\n",
    "    Args:\n",
    "    - df (pd.DataFrame) - The input dataframe object containing the classes to numerically label.\n",
    "\n",
    "    Returns:\n",
    "    - df (pd.DataFrame) - The given dataframe object after numerically labelling classes.\n",
    "    \"\"\"\n",
    "    binary_map = {\n",
    "        'I': 0, \n",
    "        'E': 1, \n",
    "        'N': 0, \n",
    "        'S': 1, \n",
    "        'F': 0, \n",
    "        'T': 1, \n",
    "        'J': 0, \n",
    "        'P': 1\n",
    "    }\n",
    "\n",
    "    df[\"EI\"] = df[\"e_vs_i\"].apply(lambda x: binary_map[str(x)])\n",
    "    df[\"SN\"] = df[\"s_vs_n\"].apply(lambda x: binary_map[str(x)])\n",
    "    df[\"TF\"] = df[\"t_vs_f\"].apply(lambda x: binary_map[str(x)])\n",
    "    df[\"JP\"] = df[\"j_vs_p\"].apply(lambda x: binary_map[str(x)])\n",
    "\n",
    "    df[\"target_vec\"] = df.apply(lambda x: [\n",
    "        x[\"EI\"],\n",
    "        x[\"SN\"],\n",
    "        x[\"TF\"],\n",
    "        x[\"JP\"]\n",
    "    ], axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def preprocess_dataset(csv_file_path = CSV_DATA_PATH):\n",
    "    \"\"\" Load and preprocess data to be used for model development.\n",
    "\n",
    "    Args:\n",
    "    - csv_file_path (str) - The file path of the csv file containing the desired data to load into a pandas DataFrame object.\n",
    "\n",
    "    Returns:\n",
    "    - numpy arrays of training and test split data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Apply preprocessing to input user posts\n",
    "    df = load_csv_data(csv_file_path)\n",
    "    df = preprocess_df(df)\n",
    "\n",
    "    # Preprocess target labels into a binary vector\n",
    "    df = binarize_targets(df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>e_vs_i</th>\n",
       "      <th>s_vs_n</th>\n",
       "      <th>t_vs_f</th>\n",
       "      <th>j_vs_p</th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "      <th>posts_no_url</th>\n",
       "      <th>posts_no_url_tokens</th>\n",
       "      <th>EI</th>\n",
       "      <th>SN</th>\n",
       "      <th>TF</th>\n",
       "      <th>JP</th>\n",
       "      <th>target_vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>J</td>\n",
       "      <td>INFJ</td>\n",
       "      <td>enfp and intj moments  https://www.youtube.com...</td>\n",
       "      <td>enfp and intj moment sportscenter not top ten ...</td>\n",
       "      <td>[enfp, and, intj, moment, sportscenter, not, t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>J</td>\n",
       "      <td>INFJ</td>\n",
       "      <td>What has been the most life-changing experienc...</td>\n",
       "      <td>what ha been the most life-changing experience...</td>\n",
       "      <td>[what, ha, been, the, most, life-changing, exp...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>J</td>\n",
       "      <td>INFJ</td>\n",
       "      <td>http://www.youtube.com/watch?v=vXZeYwwRDw8   h...</td>\n",
       "      <td>on repeat for most of today eostokendot</td>\n",
       "      <td>[on, repeat, for, most, of, today, eostokendot]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>J</td>\n",
       "      <td>INFJ</td>\n",
       "      <td>May the PerC Experience immerse you.</td>\n",
       "      <td>may the perc experience immerse you eostokendot</td>\n",
       "      <td>[may, the, perc, experience, immerse, you, eos...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>J</td>\n",
       "      <td>INFJ</td>\n",
       "      <td>The last thing my INFJ friend posted on his fa...</td>\n",
       "      <td>the last thing my infj friend posted on his fa...</td>\n",
       "      <td>[the, last, thing, my, infj, friend, posted, o...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  e_vs_i s_vs_n t_vs_f j_vs_p  type  \\\n",
       "2      I      N      F      J  INFJ   \n",
       "3      I      N      F      J  INFJ   \n",
       "4      I      N      F      J  INFJ   \n",
       "5      I      N      F      J  INFJ   \n",
       "6      I      N      F      J  INFJ   \n",
       "\n",
       "                                               posts  \\\n",
       "2  enfp and intj moments  https://www.youtube.com...   \n",
       "3  What has been the most life-changing experienc...   \n",
       "4  http://www.youtube.com/watch?v=vXZeYwwRDw8   h...   \n",
       "5               May the PerC Experience immerse you.   \n",
       "6  The last thing my INFJ friend posted on his fa...   \n",
       "\n",
       "                                        posts_no_url  \\\n",
       "2  enfp and intj moment sportscenter not top ten ...   \n",
       "3  what ha been the most life-changing experience...   \n",
       "4            on repeat for most of today eostokendot   \n",
       "5    may the perc experience immerse you eostokendot   \n",
       "6  the last thing my infj friend posted on his fa...   \n",
       "\n",
       "                                 posts_no_url_tokens  EI  SN  TF  JP  \\\n",
       "2  [enfp, and, intj, moment, sportscenter, not, t...   0   0   0   0   \n",
       "3  [what, ha, been, the, most, life-changing, exp...   0   0   0   0   \n",
       "4    [on, repeat, for, most, of, today, eostokendot]   0   0   0   0   \n",
       "5  [may, the, perc, experience, immerse, you, eos...   0   0   0   0   \n",
       "6  [the, last, thing, my, infj, friend, posted, o...   0   0   0   0   \n",
       "\n",
       "     target_vec  \n",
       "2  [0, 0, 0, 0]  \n",
       "3  [0, 0, 0, 0]  \n",
       "4  [0, 0, 0, 0]  \n",
       "5  [0, 0, 0, 0]  \n",
       "6  [0, 0, 0, 0]  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = preprocess_dataset()\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following preprocessing steps were applied to the dataset:\n",
    "- Converting letters to lowercase\n",
    "- Removing links\n",
    "- Removing punctuations\n",
    "- Removing stopwords\n",
    "\n",
    "Lematization was also applied after the above steps have been conducted, and the resulting text for each post tokenized."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word Embeddings and Text Vectorization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_myers_briggs_personalities",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
