{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Development\n",
    "\n",
    "Based on the previous work in the other notebooks, we managed to gain some initial insight for what might be contributing features when it comes to determining an online users MBTI personality factors based on what they posted, such as the overall sentiment of their posts, length of their posts, noun and verb frequency of their posts, etc. Here we try to develop a model with more emphasis on performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE + Word2Vec + Logistic Regression\n",
    "\n",
    "In the paper *Ryan, G.; Katarina, P.; Suhartono, D. MBTI Personality Prediction Using Machine Learning and SMOTE for Balancing Data Based on Statement Sentences. Information 2023, 14, 217*, a combination of SMOTE oversampling, Word2Vec word embeddings and Logistic Regression was used to achieve an average F1 - Score of 0.8337 across the four dimensions of MBTI personalities, namely:\n",
    "- Extraversion vs Introversion\n",
    "- Sensing vs Intuition\n",
    "- Thinking vs Feeling\n",
    "- Judgment vs Perception\n",
    "\n",
    "In this section we will attempt to implement this model from the paper using their methodology."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from itertools import zip_longest\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_DATA_PATH = os.path.join(\"data\", \"mbti_1.csv\")\n",
    "\n",
    "def load_csv_data(csv_file_path: str):\n",
    "    \"\"\" Load data from a given csv file into a pandas DataFrame object.\n",
    "\n",
    "    Args:\n",
    "    - csv_file_path (str) - The file path of the csv file containing the desired data to load into a pandas DataFrame object.\n",
    "\n",
    "    Returns:\n",
    "    - data (pandas.DataFrame) - The loaded data as a pandas DataFrame object.\n",
    "    \"\"\"\n",
    "    assert csv_file_path.endswith(\".csv\")\n",
    "\n",
    "    return pd.read_csv(csv_file_path)\n",
    "\n",
    "def get_online_posts_df(df: pd.DataFrame, type_col: str = \"type\", posts_col: str = \"posts\"):\n",
    "    \"\"\" Takes in an input pandas DataFrame, each row having a (user_personality_type, user_recent_50_comments) schema and user_recent_50_comments is a '|||' delimited string.\n",
    "    Outputs a DataFrame object where each row is a (user_personality_type, user_comment), where each row no contains exactly one comment.\n",
    "\n",
    "    Args:\n",
    "    - df (pandas.DataFrame) - The input pandas DataFrame object, where each row follows the schema (user_personality_type, user_recent_50_comments).\n",
    "    - type_col (str) - The column name of the input DataFrame's personality type column, which contains the user's personality type.\n",
    "    - posts_col (str) - The column name of the input DataFrame's posts column, which contains the user's recent 50 comments/online posts\n",
    "\n",
    "    Returns:\n",
    "    - output_df (pandas.DataFrame) - The output pandas DataFrame object, where each row follows the schema (user_personality_type, user_comment).\n",
    "    \"\"\"\n",
    "    online_posts_dict = {\n",
    "        type_col: [],\n",
    "        posts_col: []\n",
    "    }\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        personality_type = row[type_col]\n",
    "        recent_50_comments = row[posts_col].split(\"|||\")\n",
    "\n",
    "        online_posts_dict[type_col] += [personality_type] * len(recent_50_comments)\n",
    "        online_posts_dict[posts_col] += recent_50_comments\n",
    "\n",
    "    columns = online_posts_dict.keys()\n",
    "    values = list(zip_longest(*online_posts_dict.values()))\n",
    "    output_df = pd.DataFrame(values, columns=columns)\n",
    "\n",
    "    return output_df\n",
    "\n",
    "def get_personality_dimensions(personality_dataset: pd.DataFrame, personalities_col_name: str = \"type\"):\n",
    "    \"\"\" Return a dictionary containing the personality dimensions from the given pandas DataFrame containing Myers Briggs personalities.\n",
    "\n",
    "    Args:\n",
    "    - personality_dataset (pandas.DataFrame) - The pandas DataFrame object containing the Myers Briggs personalities data.\n",
    "    - personalities_col_name (str) - The name of the column which contains the Myers Briggs personalities in the given DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - personality_factors_dict (dict) - A dictionary containing the number personality dimensions, split into Extraversion vs Introversion, Sensing vs Intuition, Thinking vs Feeling, Judging vs Perceiving.\n",
    "    \"\"\"\n",
    "    personality_factors_dict = {\n",
    "        \"e_vs_i\": [],\n",
    "        \"s_vs_n\": [],\n",
    "        \"t_vs_f\": [],\n",
    "        \"j_vs_p\": []\n",
    "    }\n",
    "\n",
    "    for index, row in personality_dataset.iterrows():\n",
    "        personality_type_str = row[personalities_col_name]\n",
    "        e_vs_i = personality_type_str[0]\n",
    "        s_vs_n = personality_type_str[1]\n",
    "        t_vs_f = personality_type_str[2]\n",
    "        j_vs_p = personality_type_str[3]\n",
    "\n",
    "        personality_factors_dict[\"e_vs_i\"].append(e_vs_i)\n",
    "        personality_factors_dict[\"s_vs_n\"].append(s_vs_n)\n",
    "        personality_factors_dict[\"t_vs_f\"].append(t_vs_f)\n",
    "        personality_factors_dict[\"j_vs_p\"].append(j_vs_p)\n",
    "\n",
    "    return personality_factors_dict  \n",
    "\n",
    "def preprocess_df(df: pd.DataFrame):\n",
    "    \"\"\" Helper function for preprocessing the input pandas dataframe by helping clean up input user posts.\n",
    "\n",
    "    Args:\n",
    "    - df (pd.DataFrame) - The input dataframe object containing the user posts to clean up.\n",
    "\n",
    "    Returns:\n",
    "    - df (pd.DataFrame) - The given dataframe object after cleaning has been applied to the user posts data.\n",
    "    \"\"\"   \n",
    "\n",
    "    # Make each data record a user post rather than a user's top 50 posts.\n",
    "    df = get_online_posts_df(df)\n",
    "\n",
    "    # Extract the labels across the four dimensions of MBTI personalities corresponding to each post\n",
    "    personality_factors_per_post_dict = get_personality_dimensions(df)\n",
    "    columns = personality_factors_per_post_dict.keys()\n",
    "    values = list(zip_longest(*personality_factors_per_post_dict.values()))\n",
    "    personality_factors_per_post = pd.DataFrame(values, columns=columns)\n",
    "    df = pd.concat([personality_factors_per_post, df], axis=1)\n",
    "\n",
    "    # Remove urls\n",
    "    regex_pattern = r\"((?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’])))\"\n",
    "    df[\"posts_no_url\"] = df[\"posts\"].str.replace(regex_pattern, \"\").str.strip()\n",
    "\n",
    "    # Keep the End Of Sentence characters\n",
    "    df[\"posts_no_url\"] = df[\"posts_no_url\"].apply(lambda x: re.sub(r'\\.', ' EOSTokenDot ', str(x) + \" \"))\n",
    "    df[\"posts_no_url\"] = df[\"posts_no_url\"].apply(lambda x: re.sub(r'\\?', ' EOSTokenQuest ', str(x) + \" \"))\n",
    "    df[\"posts_no_url\"] = df[\"posts_no_url\"].apply(lambda x: re.sub(r'!', ' EOSTokenExs ', str(x) + \" \"))\n",
    "    \n",
    "    # Strip Punctation\n",
    "    df[\"posts_no_url\"] = df[\"posts_no_url\"].apply(lambda x: re.sub(r'[\\.+]', \".\", str(x)))\n",
    "\n",
    "    # Remove multiple fullstops\n",
    "    # df[\"posts_no_url\"] = df[\"posts_no_url\"].apply(lambda x: re.sub(r'[^\\w\\s]','', str(x)))\n",
    "\n",
    "    # Remove Non-words\n",
    "    # df[\"posts_no_url\"] = df[\"posts_no_url\"].apply(lambda x: re.sub(r'[^a-zA-Z\\s]','', str(x)))\n",
    "\n",
    "    # Convert posts to lowercase\n",
    "    df[\"posts_no_url\"] = df[\"posts_no_url\"].apply(lambda x: str(x).lower())\n",
    "\n",
    "    # Remove multiple letter repeating words\n",
    "    # df[\"posts_no_url\"] = df[\"posts_no_url\"].apply(lambda x: re.sub(r'([a-z])\\1{2,}[\\s|\\w]*','', str(x)))\n",
    "    \n",
    "    # Strip trailing whitespaces\n",
    "    df[\"posts_no_url\"] = df[\"posts_no_url\"].apply(lambda x: str(x).strip())\n",
    "\n",
    "    # Remove rows with no text\n",
    "    df[\"posts_no_url\"] = df[\"posts_no_url\"].replace('', np.nan)\n",
    "    df[\"posts_no_url\"] = df[\"posts_no_url\"].replace('nan', np.nan)\n",
    "    df[\"posts_no_url\"] = df[\"posts_no_url\"].replace(\"'\", np.nan)\n",
    "    df[\"posts_no_url\"] = df[\"posts_no_url\"].replace(\"''\", np.nan)\n",
    "    df[\"posts_no_url\"] = df[\"posts_no_url\"].replace('\"', np.nan)\n",
    "    df[\"posts_no_url\"] = df[\"posts_no_url\"].replace('\"\"', np.nan)\n",
    "    df.dropna(subset=[\"posts_no_url\"], inplace=True)\n",
    "\n",
    "    # Tokenize posts\n",
    "    df[\"posts_no_url_tokens\"] = df[\"posts_no_url\"].apply(wordpunct_tokenize)\n",
    "\n",
    "    # Remove stop words\n",
    "    df[\"posts_no_url_tokens_no_stop\"] = df[\"posts_no_url_tokens\"].apply(lambda x: [token for token in x if token not in stop_words])\n",
    "\n",
    "    # Lemmatize posts\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    df[\"posts_no_url_tokens_no_stop\"] = df[\"posts_no_url_tokens_no_stop\"].apply(lambda x: [lemmatizer.lemmatize(token) for token in x])\n",
    "\n",
    "    return df[[\"e_vs_i\", \"s_vs_n\", \"t_vs_f\", \"j_vs_p\", \"type\", \"posts\", \"posts_no_url\", \"posts_no_url_tokens_no_stop\"]]\n",
    "\n",
    "def binarize_targets(df: pd.DataFrame):\n",
    "    \"\"\" Apply 0/1 labels to the input classes in the df.\n",
    "\n",
    "    Args:\n",
    "    - df (pd.DataFrame) - The input dataframe object containing the classes to numerically label.\n",
    "\n",
    "    Returns:\n",
    "    - df (pd.DataFrame) - The given dataframe object after numerically labelling classes.\n",
    "    \"\"\"\n",
    "    binary_map = {\n",
    "        'I': 0, \n",
    "        'E': 1, \n",
    "        'N': 0, \n",
    "        'S': 1, \n",
    "        'F': 0, \n",
    "        'T': 1, \n",
    "        'J': 0, \n",
    "        'P': 1\n",
    "    }\n",
    "\n",
    "    df[\"EI\"] = df[\"e_vs_i\"].apply(lambda x: binary_map[str(x)])\n",
    "    df[\"SN\"] = df[\"s_vs_n\"].apply(lambda x: binary_map[str(x)])\n",
    "    df[\"TF\"] = df[\"t_vs_f\"].apply(lambda x: binary_map[str(x)])\n",
    "    df[\"JP\"] = df[\"j_vs_p\"].apply(lambda x: binary_map[str(x)])\n",
    "\n",
    "    df[\"target_vec\"] = df.apply(lambda x: [\n",
    "        x[\"EI\"],\n",
    "        x[\"SN\"],\n",
    "        x[\"TF\"],\n",
    "        x[\"JP\"]\n",
    "    ], axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def preprocess_dataset(csv_file_path: str = CSV_DATA_PATH):\n",
    "    \"\"\" Load and preprocess data to be used for model development.\n",
    "\n",
    "    Args:\n",
    "    - csv_file_path (str) - The file path of the csv file containing the desired data to load into a pandas DataFrame object.\n",
    "\n",
    "    Returns:\n",
    "    - numpy arrays of training and test split data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Apply preprocessing to input user posts\n",
    "    df = load_csv_data(csv_file_path)\n",
    "    df = preprocess_df(df)\n",
    "\n",
    "    # Preprocess target labels into a binary vector\n",
    "    df = binarize_targets(df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocess_dataset()\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following preprocessing steps were applied to the dataset:\n",
    "- Converting letters to lowercase\n",
    "- Removing links\n",
    "- Removing punctuations\n",
    "- Removing stopwords\n",
    "\n",
    "Lematization was also applied after the above steps have been conducted, and the resulting text for each post tokenized."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word Embeddings and Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Word2Vec model\n",
    "w2w_model = Word2Vec(sentences=df[\"posts_no_url_tokens_no_stop\"], vector_size=500, window=5, min_count=5, epochs=50)\n",
    "\n",
    "def get_train_test_split(training_fraction: float, df: pd.DataFrame, mbti_dim: str):\n",
    "    \"\"\" Get a training and test dataset split.\n",
    "\n",
    "    Args:\n",
    "    - training_proportion (float) - The fraction of the dataset to be used as training data.\n",
    "    - df (pd.DataFrame) - The pandas dataframe object containing the data to be split into train and test sets.\n",
    "    - mbti_dim (str) - The column name of the target MBTI personality dimension.\n",
    "\n",
    "    Returns:\n",
    "    - Train and test datasets where the target variable is the desired MBTI personality dimension (mbti_dim).\n",
    "    \"\"\"\n",
    "\n",
    "    # Split into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df[\"posts_no_url_tokens_no_stop\"], df[mbti_dim], test_size=(1 - training_fraction), random_state=42)\n",
    "\n",
    "    X_train = np.array([vectorize(post) for post in X_train])\n",
    "    X_test = np.array([vectorize(post) for post in X_test])\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def vectorize(post: list):\n",
    "    \"\"\" Vectorize a sentence to a vector representation using Word2Vec CBOW.\n",
    "\n",
    "    Args:\n",
    "    - post (str) - A post to vectorize. Here, the post is passed into the function as a list of word tokens.\n",
    "\n",
    "    Returns:\n",
    "    - Word2Vec word embedding (CBOW).\n",
    "    \"\"\"\n",
    "\n",
    "    # Vectorize sentence\n",
    "    words_vector = [w2w_model.wv[token] for token in post if token in w2w_model.wv]\n",
    "\n",
    "    if len(words_vector) == 0:\n",
    "        return np.zeros(500)\n",
    "    \n",
    "    words_vector = np.array(words_vector)\n",
    "    return words_vector.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_EI, X_test_EI, y_train_EI, y_test_EI = get_train_test_split(0.7, df, \"EI\") \n",
    "print(X_train_EI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test_EI[160])\n",
    "print(type(X_test_EI))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create functionality that can split out user posts dataset into train and test sets, and vectorize the text data to be used as model inputs as Word2Vec word embeddings. The benefit with using word embeddings is that we can represent words in a numerical format (ML/DL models can only take in numerical input) that allows words with similar meanings to have the same representation, i.e. smilarity of meaning between words is captured in the word embeddings."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, log_loss\n",
    "from sklearn.model_selection import LearningCurveDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_LR():\n",
    "    \"\"\" Create a simple Logistic Regression model on the given training data. \n",
    "    SMOTE oversampling will also performed when this model is fitted to training data.\n",
    "\n",
    "    Args:\n",
    "    - X (iterable) - Input training examples.\n",
    "    - y (iterable) - Training target labels.\n",
    "\n",
    "    Returns:\n",
    "    - The Logistic Regression model to be fitted onto training data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Instatiate SMOTE over-sampler and Logistic Regression model.\n",
    "    sampler = SMOTE(random_state=42)\n",
    "    lr = LogisticRegression(\n",
    "        penalty=None,\n",
    "        max_iter=1000,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Build pipeline/model.\n",
    "    pipeline = Pipeline([\n",
    "        (\"sampler\", sampler),\n",
    "        (\"lr\", lr)\n",
    "    ])\n",
    "\n",
    "    return pipeline\n",
    "\n",
    "def train_model(X, y, model):\n",
    "    \"\"\" Train the input model on the given training data.\n",
    "\n",
    "    Args:\n",
    "    - X (iterable) - Input training examples.\n",
    "    - y (iterable) - Training target labels.\n",
    "    - model - The model to be fitted onto the training data.\n",
    "\n",
    "    Returns:\n",
    "    - The model fitted to the training data.\n",
    "    \"\"\"\n",
    "    model.fit(X, y)\n",
    "\n",
    "    return model\n",
    "\n",
    "def evaluate(X, y, model, target_names):\n",
    "    \"\"\" Evaluate the given trained model on the test data.\n",
    "\n",
    "    Args:\n",
    "    - X (iterable) - Input test examples.\n",
    "    - y (iterable) - Test target labels.\n",
    "    - target_names - The names of the labels for the classfication report.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(X)\n",
    "    print(classification_report(y, y_pred, target_names=target_names)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets for each MBTI dimension.\n",
    "X_train_EI, X_test_EI, y_train_EI, y_test_EI = get_train_test_split(0.7, df, \"EI\")\n",
    "X_train_SN, X_test_SN, y_train_SN, y_test_SN = get_train_test_split(0.7, df, \"SN\")\n",
    "X_train_TF, X_test_TF, y_train_TF, y_test_TF = get_train_test_split(0.7, df, \"TF\") \n",
    "X_train_JP, X_test_JP, y_train_JP, y_test_JP = get_train_test_split(0.7, df, \"JP\")\n",
    "\n",
    "# Create Logistic Regression classifiers for each MBTI dimension.\n",
    "ei_classifier = model_LR()\n",
    "sn_classifier = model_LR()\n",
    "tf_classifier = model_LR()\n",
    "jp_classifier = model_LR()\n",
    "\n",
    "# Train classifiers.\n",
    "ei_classifier = train_model(X_train_EI, y_train_EI, ei_classifier)\n",
    "sn_classifier = train_model(X_train_SN, y_train_SN, sn_classifier)\n",
    "tf_classifier = train_model(X_train_TF, y_train_TF, tf_classifier)\n",
    "jp_classifier = train_model(X_train_JP, y_train_JP, jp_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Extraversion vs Introversion classifier.\n",
    "evaluate(X_test_EI, y_test_EI, ei_classifier, [\"Introversion\", \"Extraversion\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Sensing vs Intuition classifier.\n",
    "evaluate(X_test_SN, y_test_SN, sn_classifier, [\"Intuition\", \"Sensing\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Thinking vs Feeling classifier.\n",
    "evaluate(X_test_TF, y_test_TF, tf_classifier, [\"Feeling\", \"Thinking\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Judging vs Perceiving classifier.\n",
    "evaluate(X_test_JP, y_test_JP, jp_classifier, [\"Judging\", \"Perceiving\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning cruves for the classifiers.\n",
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(15,10))\n",
    "ax = ax.flatten()\n",
    "\n",
    "# Extraversion vs Introversion.\n",
    "X_EI = np.concatenate((X_train_EI, X_test_EI), axis=0)\n",
    "y_EI = np.concatenate((y_train_EI, y_test_EI), axis=0)\n",
    "model0 = model_LR()\n",
    "LearningCurveDisplay.from_estimator(model0, X_EI, y_EI, scoring=\"f1_macro\", score_name=\"F1 Macro Avg\", score_type=\"both\", n_jobs=4, line_kw={\"marker\": \"o\"}, random_state=42, ax=ax[0])\n",
    "handles0, label0 = ax[0].get_legend_handles_labels()\n",
    "ax[0].legend(handles0[:2], [\"Training Score\", \"Test Score\"])\n",
    "ax[0].set_title(\"Extraversion vs Introversion\")\n",
    "\n",
    "# Sensing vs Intuition.\n",
    "X_SN = np.concatenate((X_train_SN, X_test_SN), axis=0)\n",
    "y_SN = np.concatenate((y_train_SN, y_test_SN), axis=0)\n",
    "model1 = model_LR()\n",
    "LearningCurveDisplay.from_estimator(model1, X_SN, y_SN, scoring=\"f1_macro\", score_name=\"F1 Macro Avg\", score_type=\"both\", n_jobs=4, line_kw={\"marker\": \"o\"}, random_state=42, ax=ax[1])\n",
    "handles1, label1 = ax[1].get_legend_handles_labels()\n",
    "ax[1].legend(handles1[:2], [\"Training Score\", \"Test Score\"])\n",
    "ax[1].set_title(\"Sensing vs Intuition\")\n",
    "\n",
    "# Thinking vs Feeling.\n",
    "X_TF = np.concatenate((X_train_TF, X_test_TF), axis=0)\n",
    "y_TF = np.concatenate((y_train_TF, y_test_TF), axis=0)\n",
    "model2 = model_LR()\n",
    "LearningCurveDisplay.from_estimator(model2, X_TF, y_TF, scoring=\"f1_macro\", score_name=\"F1 Macro Avg\", score_type=\"both\", n_jobs=4, line_kw={\"marker\": \"o\"}, random_state=42, ax=ax[2])\n",
    "handles2, label2 = ax[2].get_legend_handles_labels()\n",
    "ax[2].legend(handles2[:2], [\"Training Score\", \"Test Score\"])\n",
    "ax[2].set_title(\"Thinking vs Feeling\")\n",
    "\n",
    "# Judging vs Perceiving.\n",
    "X_JP = np.concatenate((X_train_JP, X_test_JP), axis=0)\n",
    "y_JP = np.concatenate((y_train_JP, y_test_JP), axis=0)\n",
    "model3 = model_LR()\n",
    "LearningCurveDisplay.from_estimator(model3, X_JP, y_JP, scoring=\"f1_macro\", score_name=\"F1 Macro Avg\", score_type=\"both\", n_jobs=4, line_kw={\"marker\": \"o\"}, random_state=42, ax=ax[3])\n",
    "handles3, label3 = ax[3].get_legend_handles_labels()\n",
    "ax[3].legend(handles3[:2], [\"Training Score\", \"Test Score\"])\n",
    "ax[3].set_title(\"Judging vs Perceiving\")\n",
    "\n",
    "fig.suptitle(\"Learning Curves\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the classification reports we see that using the default parameter settings for logistic regression classifiers gives us an macro average F1 score in the 50s across the four dimensions of mbti personality. The learning curves above also show that overfitting might is not that big given the volume of posts we have in our dataset, however, it could also be the default L2 regularisation parameter of 10 in sklearn's Logistic Regression object.\n",
    "\n",
    "Now we will try to experiment with some different parameter settings to see if we can improve performance (with respect to macro average F1) and see if we can get closer to the performance acheived in the paper *Ryan, G.; Katarina, P.; Suhartono, D. MBTI Personality Prediction Using Machine Learning and SMOTE for Balancing Data Based on Statement Sentences. Information 2023, 14, 217*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_LR_CV():\n",
    "    \"\"\" Create Logistic Regression model on the given training data. \n",
    "    \n",
    "    Cross Validation will be used to tune the hyperparameters of the model and SMOTE oversampling will also performed when this model is fitted to training data.\n",
    "\n",
    "    Args:\n",
    "    - X (iterable) - Input training examples.\n",
    "    - y (iterable) - Training target labels.\n",
    "\n",
    "    Returns:\n",
    "    - The Logistic Regression model to be fitted onto training data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Instatiate SMOTE over-sampler and Logistic Regression model.\n",
    "    sampler = SMOTE(random_state=42)\n",
    "    lr = LogisticRegressionCV(\n",
    "        Cs=[0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "        scoring=\"f1_macro\",\n",
    "        n_jobs=-1,\n",
    "        max_iter=1000,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Build pipeline/model.\n",
    "    pipeline = Pipeline([\n",
    "        (\"sampler\", sampler),\n",
    "        (\"lr\", lr)\n",
    "    ])\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets for each MBTI dimension.\n",
    "X_train_EI, X_test_EI, y_train_EI, y_test_EI = get_train_test_split(0.7, df, \"EI\")\n",
    "X_train_SN, X_test_SN, y_train_SN, y_test_SN = get_train_test_split(0.7, df, \"SN\")\n",
    "X_train_TF, X_test_TF, y_train_TF, y_test_TF = get_train_test_split(0.7, df, \"TF\") \n",
    "X_train_JP, X_test_JP, y_train_JP, y_test_JP = get_train_test_split(0.7, df, \"JP\")\n",
    "\n",
    "# Create Logistic Regression classifiers for each MBTI dimension.\n",
    "ei_classifier = model_LR_CV()\n",
    "sn_classifier = model_LR_CV()\n",
    "tf_classifier = model_LR_CV()\n",
    "jp_classifier = model_LR_CV()\n",
    "\n",
    "# Train classifiers.\n",
    "ei_classifier = train_model(X_train_EI, y_train_EI, ei_classifier)\n",
    "sn_classifier = train_model(X_train_SN, y_train_SN, sn_classifier)\n",
    "tf_classifier = train_model(X_train_TF, y_train_TF, tf_classifier)\n",
    "jp_classifier = train_model(X_train_JP, y_train_JP, jp_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Extraversion vs Introversion classifier.\n",
    "evaluate(X_test_EI, y_test_EI, ei_classifier, [\"Introversion\", \"Extraversion\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Sensing vs Intuition classifier.\n",
    "evaluate(X_test_SN, y_test_SN, sn_classifier, [\"Intuition\", \"Sensing\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Thinking vs Feeling classifier.\n",
    "evaluate(X_test_TF, y_test_TF, tf_classifier, [\"Feeling\", \"Thinking\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Judging vs Perceiving classifier.\n",
    "evaluate(X_test_JP, y_test_JP, jp_classifier, [\"Judging\", \"Perceiving\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning cruves for the classifiers.\n",
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(15,10))\n",
    "ax = ax.flatten()\n",
    "\n",
    "# Extraversion vs Introversion.\n",
    "X_EI = np.concatenate((X_train_EI, X_test_EI), axis=0)\n",
    "y_EI = np.concatenate((y_train_EI, y_test_EI), axis=0)\n",
    "model0 = model_LR()\n",
    "LearningCurveDisplay.from_estimator(model0, X_EI, y_EI, scoring=\"f1_macro\", score_name=\"F1 Macro Avg\", score_type=\"both\", n_jobs=4, line_kw={\"marker\": \"o\"}, random_state=42, ax=ax[0])\n",
    "handles0, label0 = ax[0].get_legend_handles_labels()\n",
    "ax[0].legend(handles0[:2], [\"Training Score\", \"Test Score\"])\n",
    "ax[0].set_title(\"Extraversion vs Introversion\")\n",
    "\n",
    "# Sensing vs Intuition.\n",
    "X_SN = np.concatenate((X_train_SN, X_test_SN), axis=0)\n",
    "y_SN = np.concatenate((y_train_SN, y_test_SN), axis=0)\n",
    "model1 = model_LR()\n",
    "LearningCurveDisplay.from_estimator(model1, X_SN, y_SN, scoring=\"f1_macro\", score_name=\"F1 Macro Avg\", score_type=\"both\", n_jobs=4, line_kw={\"marker\": \"o\"}, random_state=42, ax=ax[1])\n",
    "handles1, label1 = ax[1].get_legend_handles_labels()\n",
    "ax[1].legend(handles1[:2], [\"Training Score\", \"Test Score\"])\n",
    "ax[1].set_title(\"Sensing vs Intuition\")\n",
    "\n",
    "# Thinking vs Feeling.\n",
    "X_TF = np.concatenate((X_train_TF, X_test_TF), axis=0)\n",
    "y_TF = np.concatenate((y_train_TF, y_test_TF), axis=0)\n",
    "model2 = model_LR()\n",
    "LearningCurveDisplay.from_estimator(model2, X_TF, y_TF, scoring=\"f1_macro\", score_name=\"F1 Macro Avg\", score_type=\"both\", n_jobs=4, line_kw={\"marker\": \"o\"}, random_state=42, ax=ax[2])\n",
    "handles2, label2 = ax[2].get_legend_handles_labels()\n",
    "ax[2].legend(handles2[:2], [\"Training Score\", \"Test Score\"])\n",
    "ax[2].set_title(\"Thinking vs Feeling\")\n",
    "\n",
    "# Judging vs Perceiving.\n",
    "X_JP = np.concatenate((X_train_JP, X_test_JP), axis=0)\n",
    "y_JP = np.concatenate((y_train_JP, y_test_JP), axis=0)\n",
    "model3 = model_LR()\n",
    "LearningCurveDisplay.from_estimator(model3, X_JP, y_JP, scoring=\"f1_macro\", score_name=\"F1 Macro Avg\", score_type=\"both\", n_jobs=4, line_kw={\"marker\": \"o\"}, random_state=42, ax=ax[3])\n",
    "handles3, label3 = ax[3].get_legend_handles_labels()\n",
    "ax[3].legend(handles3[:2], [\"Training Score\", \"Test Score\"])\n",
    "ax[3].set_title(\"Judging vs Perceiving\")\n",
    "\n",
    "fig.suptitle(\"Learning Curves\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Neural Network\n",
    "\n",
    "In the paper *Sakdipat Ontoum, Jonathan H. Chan. Personality Type Based on Myers-Briggs Type Indicator with Text Posting Style by using Traditional and Deep Learning. https://doi.org/10.48550/arXiv.2201.08717*, they acheived a macro average F1 score of 80+ across the following MBTI dimensions using a deep neural network:\n",
    "- Extraversion vs Introversion\n",
    "- Sensing vs Intuition\n",
    "- Thinking vs Feeling\n",
    "- Judgment vs Perception\n",
    "\n",
    "In this section we will attempt to implement this model from the paper and see if we can achieve better results than the model used in the feature engineering notebook."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\stefa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from itertools import zip_longest\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_DATA_PATH = os.path.join(\"data\", \"mbti_1.csv\")\n",
    "\n",
    "def load_csv_data(csv_file_path: str):\n",
    "    \"\"\" Load data from a given csv file into a pandas DataFrame object.\n",
    "\n",
    "    Args:\n",
    "    - csv_file_path (str) - The file path of the csv file containing the desired data to load into a pandas DataFrame object.\n",
    "\n",
    "    Returns:\n",
    "    - data (pandas.DataFrame) - The loaded data as a pandas DataFrame object.\n",
    "    \"\"\"\n",
    "    assert csv_file_path.endswith(\".csv\")\n",
    "\n",
    "    return pd.read_csv(csv_file_path)\n",
    "\n",
    "def get_online_posts_df(df: pd.DataFrame, type_col: str = \"type\", posts_col: str = \"posts\"):\n",
    "    \"\"\" Takes in an input pandas DataFrame, each row having a (user_personality_type, user_recent_50_comments) schema and user_recent_50_comments is a '|||' delimited string.\n",
    "    Outputs a DataFrame object where each row is a (user_personality_type, user_comment), where each row no contains exactly one comment.\n",
    "\n",
    "    Args:\n",
    "    - df (pandas.DataFrame) - The input pandas DataFrame object, where each row follows the schema (user_personality_type, user_recent_50_comments).\n",
    "    - type_col (str) - The column name of the input DataFrame's personality type column, which contains the user's personality type.\n",
    "    - posts_col (str) - The column name of the input DataFrame's posts column, which contains the user's recent 50 comments/online posts\n",
    "\n",
    "    Returns:\n",
    "    - output_df (pandas.DataFrame) - The output pandas DataFrame object, where each row follows the schema (user_personality_type, user_comment).\n",
    "    \"\"\"\n",
    "    online_posts_dict = {\n",
    "        type_col: [],\n",
    "        posts_col: []\n",
    "    }\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        personality_type = row[type_col]\n",
    "        recent_50_comments = row[posts_col].split(\"|||\")\n",
    "\n",
    "        online_posts_dict[type_col] += [personality_type] * len(recent_50_comments)\n",
    "        online_posts_dict[posts_col] += recent_50_comments\n",
    "\n",
    "    columns = online_posts_dict.keys()\n",
    "    values = list(zip_longest(*online_posts_dict.values()))\n",
    "    output_df = pd.DataFrame(values, columns=columns)\n",
    "\n",
    "    return output_df\n",
    "\n",
    "def get_personality_dimensions(personality_dataset: pd.DataFrame, personalities_col_name: str = \"type\"):\n",
    "    \"\"\" Return a dictionary containing the personality dimensions from the given pandas DataFrame containing Myers Briggs personalities.\n",
    "\n",
    "    Args:\n",
    "    - personality_dataset (pandas.DataFrame) - The pandas DataFrame object containing the Myers Briggs personalities data.\n",
    "    - personalities_col_name (str) - The name of the column which contains the Myers Briggs personalities in the given DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - personality_factors_dict (dict) - A dictionary containing the number personality dimensions, split into Extraversion vs Introversion, Sensing vs Intuition, Thinking vs Feeling, Judging vs Perceiving.\n",
    "    \"\"\"\n",
    "    personality_factors_dict = {\n",
    "        \"e_vs_i\": [],\n",
    "        \"s_vs_n\": [],\n",
    "        \"t_vs_f\": [],\n",
    "        \"j_vs_p\": []\n",
    "    }\n",
    "\n",
    "    for index, row in personality_dataset.iterrows():\n",
    "        personality_type_str = row[personalities_col_name]\n",
    "        e_vs_i = personality_type_str[0]\n",
    "        s_vs_n = personality_type_str[1]\n",
    "        t_vs_f = personality_type_str[2]\n",
    "        j_vs_p = personality_type_str[3]\n",
    "\n",
    "        personality_factors_dict[\"e_vs_i\"].append(e_vs_i)\n",
    "        personality_factors_dict[\"s_vs_n\"].append(s_vs_n)\n",
    "        personality_factors_dict[\"t_vs_f\"].append(t_vs_f)\n",
    "        personality_factors_dict[\"j_vs_p\"].append(j_vs_p)\n",
    "\n",
    "    return personality_factors_dict  \n",
    "\n",
    "def preprocess_df(df: pd.DataFrame):\n",
    "    \"\"\" Helper function for preprocessing the input pandas dataframe by helping clean up input user posts.\n",
    "\n",
    "    Args:\n",
    "    - df (pd.DataFrame) - The input dataframe object containing the user posts to clean up.\n",
    "\n",
    "    Returns:\n",
    "    - df (pd.DataFrame) - The given dataframe object after cleaning has been applied to the user posts data.\n",
    "    \"\"\"   \n",
    "\n",
    "    # Make each data record a user post rather than a user's top 50 posts.\n",
    "    df = get_online_posts_df(df)\n",
    "\n",
    "    # Extract the labels across the four dimensions of MBTI personalities corresponding to each post\n",
    "    personality_factors_per_post_dict = get_personality_dimensions(df)\n",
    "    columns = personality_factors_per_post_dict.keys()\n",
    "    values = list(zip_longest(*personality_factors_per_post_dict.values()))\n",
    "    personality_factors_per_post = pd.DataFrame(values, columns=columns)\n",
    "    df = pd.concat([personality_factors_per_post, df], axis=1)\n",
    "\n",
    "    # Remove urls\n",
    "    regex_pattern = r\"((?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’])))\"\n",
    "    df[\"posts_no_url\"] = df[\"posts\"].str.replace(regex_pattern, \"\").str.strip()\n",
    "\n",
    "    # Keep the End Of Sentence characters\n",
    "    df[\"posts_no_url\"] = df[\"posts_no_url\"].apply(lambda x: re.sub(r'\\.', ' EOSTokenDot ', str(x) + \" \"))\n",
    "    df[\"posts_no_url\"] = df[\"posts_no_url\"].apply(lambda x: re.sub(r'\\?', ' EOSTokenQuest ', str(x) + \" \"))\n",
    "    df[\"posts_no_url\"] = df[\"posts_no_url\"].apply(lambda x: re.sub(r'!', ' EOSTokenExs ', str(x) + \" \"))\n",
    "\n",
    "    # Strip trailing whitespaces\n",
    "    df[\"posts_no_url\"] = df[\"posts_no_url\"].apply(lambda x: str(x).strip())\n",
    "\n",
    "    # Tokenize posts\n",
    "    df[\"posts_no_url_tokens\"] = df[\"posts_no_url\"].apply(word_tokenize)\n",
    "\n",
    "    # Remove stop words\n",
    "    df[\"posts_no_url_tokens_no_stop\"] = df[\"posts_no_url_tokens\"].apply(lambda x: [token for token in x if token not in stop_words])\n",
    "\n",
    "    # Lemmatize posts\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    df[\"posts_no_url_tokens_no_stop\"] = df[\"posts_no_url_tokens_no_stop\"].apply(lambda x: [lemmatizer.lemmatize(token) for token in x])\n",
    "\n",
    "    # Remove rows with no text\n",
    "    df[\"posts_no_url\"] = df[\"posts_no_url\"].replace('nan', np.nan)\n",
    "    df[\"posts_no_url\"] = df[\"posts_no_url\"].replace('', np.nan)\n",
    "    df.dropna(subset=[\"posts_no_url\"], inplace=True)\n",
    "\n",
    "    # Create column for preprocessed posts\n",
    "    df[\"preprocessed_posts\"] = df[\"posts_no_url_tokens_no_stop\"].apply(lambda x: \" \".join(x))\n",
    "\n",
    "    return df[[\"e_vs_i\", \"s_vs_n\", \"t_vs_f\", \"j_vs_p\", \"type\", \"posts\", \"preprocessed_posts\"]]\n",
    "\n",
    "def binarize_targets(df: pd.DataFrame):\n",
    "    \"\"\" Apply 0/1 labels to the input classes in the df.\n",
    "\n",
    "    Args:\n",
    "    - df (pd.DataFrame) - The input dataframe object containing the classes to numerically label.\n",
    "\n",
    "    Returns:\n",
    "    - df (pd.DataFrame) - The given dataframe object after numerically labelling classes.\n",
    "    \"\"\"\n",
    "    binary_map = {\n",
    "        'I': 0, \n",
    "        'E': 1, \n",
    "        'N': 0, \n",
    "        'S': 1, \n",
    "        'F': 0, \n",
    "        'T': 1, \n",
    "        'J': 0, \n",
    "        'P': 1\n",
    "    }\n",
    "\n",
    "    df[\"EI\"] = df[\"e_vs_i\"].apply(lambda x: binary_map[str(x)])\n",
    "    df[\"SN\"] = df[\"s_vs_n\"].apply(lambda x: binary_map[str(x)])\n",
    "    df[\"TF\"] = df[\"t_vs_f\"].apply(lambda x: binary_map[str(x)])\n",
    "    df[\"JP\"] = df[\"j_vs_p\"].apply(lambda x: binary_map[str(x)])\n",
    "\n",
    "    df[\"target_vec\"] = df.apply(lambda x: [\n",
    "        x[\"EI\"],\n",
    "        x[\"SN\"],\n",
    "        x[\"TF\"],\n",
    "        x[\"JP\"]\n",
    "    ], axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def preprocess_dataset(csv_file_path: str = CSV_DATA_PATH):\n",
    "    \"\"\" Load and preprocess data to be used for model development.\n",
    "\n",
    "    Args:\n",
    "    - csv_file_path (str) - The file path of the csv file containing the desired data to load into a pandas DataFrame object.\n",
    "\n",
    "    Returns:\n",
    "    - numpy arrays of training and test split data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Apply preprocessing to input user posts\n",
    "    df = load_csv_data(csv_file_path)\n",
    "    df = preprocess_df(df)\n",
    "\n",
    "    # Preprocess target labels into a binary vector\n",
    "    df = binarize_targets(df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stefa\\AppData\\Local\\Temp\\ipykernel_20492\\1312023727.py:99: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df[\"posts_no_url\"] = df[\"posts\"].str.replace(regex_pattern, \"\").str.strip()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>e_vs_i</th>\n",
       "      <th>s_vs_n</th>\n",
       "      <th>t_vs_f</th>\n",
       "      <th>j_vs_p</th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "      <th>preprocessed_posts</th>\n",
       "      <th>EI</th>\n",
       "      <th>SN</th>\n",
       "      <th>TF</th>\n",
       "      <th>JP</th>\n",
       "      <th>target_vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>J</td>\n",
       "      <td>INFJ</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw</td>\n",
       "      <td>'</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>J</td>\n",
       "      <td>INFJ</td>\n",
       "      <td>enfp and intj moments  https://www.youtube.com...</td>\n",
       "      <td>enfp intj moment sportscenter top ten play prank</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>J</td>\n",
       "      <td>INFJ</td>\n",
       "      <td>What has been the most life-changing experienc...</td>\n",
       "      <td>What life-changing experience life EOSTokenQuest</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>J</td>\n",
       "      <td>INFJ</td>\n",
       "      <td>http://www.youtube.com/watch?v=vXZeYwwRDw8   h...</td>\n",
       "      <td>On repeat today EOSTokenDot</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>J</td>\n",
       "      <td>INFJ</td>\n",
       "      <td>May the PerC Experience immerse you.</td>\n",
       "      <td>May PerC Experience immerse EOSTokenDot</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  e_vs_i s_vs_n t_vs_f j_vs_p  type  \\\n",
       "0      I      N      F      J  INFJ   \n",
       "2      I      N      F      J  INFJ   \n",
       "3      I      N      F      J  INFJ   \n",
       "4      I      N      F      J  INFJ   \n",
       "5      I      N      F      J  INFJ   \n",
       "\n",
       "                                               posts  \\\n",
       "0        'http://www.youtube.com/watch?v=qsXHcwe3krw   \n",
       "2  enfp and intj moments  https://www.youtube.com...   \n",
       "3  What has been the most life-changing experienc...   \n",
       "4  http://www.youtube.com/watch?v=vXZeYwwRDw8   h...   \n",
       "5               May the PerC Experience immerse you.   \n",
       "\n",
       "                                 preprocessed_posts  EI  SN  TF  JP  \\\n",
       "0                                                 '   0   0   0   0   \n",
       "2  enfp intj moment sportscenter top ten play prank   0   0   0   0   \n",
       "3  What life-changing experience life EOSTokenQuest   0   0   0   0   \n",
       "4                       On repeat today EOSTokenDot   0   0   0   0   \n",
       "5           May PerC Experience immerse EOSTokenDot   0   0   0   0   \n",
       "\n",
       "     target_vec  \n",
       "0  [0, 0, 0, 0]  \n",
       "2  [0, 0, 0, 0]  \n",
       "3  [0, 0, 0, 0]  \n",
       "4  [0, 0, 0, 0]  \n",
       "5  [0, 0, 0, 0]  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = preprocess_dataset()\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following preprocessing steps were applied to the dataset:\n",
    "- Removing links\n",
    "- Removing stopwords\n",
    "- Lemmatization of text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train keras tokenizer on the posts\n",
    "tokenizer = Tokenizer(num_words=72000)\n",
    "tokenizer.fit_on_texts(df[\"preprocessed_posts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_posts(posts, tokenizer, max_sequence_length: int):\n",
    "    \"\"\" Turns the posts into vectorized padded sequences.\n",
    "\n",
    "    Args:\n",
    "    - posts (iterable) - An iterable object containing the posts to preprocess.\n",
    "    - tokenizer - A tokenizer to tokenize the posts.\n",
    "    - max_sequence_length (int) - The maximum length of the padded sequences returned after tokenization.\n",
    "\n",
    "    Returns:\n",
    "    - The padded sequences after tokenization.\n",
    "    \"\"\"\n",
    "    post_sequences = tokenizer.texts_to_sequences(posts)\n",
    "    return pad_sequences(post_sequences, maxlen=max_sequence_length)\n",
    "\n",
    "def get_train_test_split(training_fraction: float, df: pd.DataFrame, mbti_dim: str, preprocessed_posts):\n",
    "    \"\"\" Get a training and test dataset split.\n",
    "\n",
    "    Args:\n",
    "    - training_proportion (float) - The fraction of the dataset to be used as training data.\n",
    "    - df (pd.DataFrame) - The pandas dataframe object containing the data to be split into train and test sets.\n",
    "    - mbti_dim (str) - The column name of the target MBTI personality dimension.\n",
    "    - preprocessed_posts (iterable) - The preprocessed posts.\n",
    "\n",
    "    Returns:\n",
    "    - Train and test datasets where the target variable is the desired MBTI personality dimension (mbti_dim).\n",
    "    \"\"\"\n",
    "\n",
    "    # Split into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(preprocessed_posts, df[mbti_dim], test_size=(1 - training_fraction), random_state=42)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the posts.\n",
    "preprocessed_posts = preprocess_posts(df[\"preprocessed_posts\"], tokenizer, 200)\n",
    "\n",
    "# Split data into train and test sets for each MBTI dimension.\n",
    "X_train_EI, X_test_EI, y_train_EI, y_test_EI = get_train_test_split(0.7, df, \"EI\", preprocessed_posts)\n",
    "X_train_SN, X_test_SN, y_train_SN, y_test_SN = get_train_test_split(0.7, df, \"SN\", preprocessed_posts)\n",
    "X_train_TF, X_test_TF, y_train_TF, y_test_TF = get_train_test_split(0.7, df, \"TF\", preprocessed_posts) \n",
    "X_train_JP, X_test_JP, y_train_JP, y_test_JP = get_train_test_split(0.7, df, \"JP\", preprocessed_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ...    0    0   34]\n",
      " [   0    0    0 ... 1763  281 5927]\n",
      " [   0    0    0 ...  173   45    6]\n",
      " ...\n",
      " [   0    0    0 ...    1    1    1]\n",
      " [   0    0    0 ...    1    1    1]\n",
      " [   0    0    0 ...   57    1   34]]\n"
     ]
    }
   ],
   "source": [
    "print(preprocessed_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0 392   7 974  54   7 942 405  13 320 168   6  20 227 960   1\n",
      "   1   1]\n",
      "411731\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "print(preprocessed_posts[160])\n",
    "print(len(preprocessed_posts))\n",
    "print(len(preprocessed_posts[160]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0     0 ...     1     1     1]\n",
      " [    0     0     0 ...     1     1     1]\n",
      " [    0     0     0 ...     1     1     1]\n",
      " ...\n",
      " [    0     0     0 ...     1     1     1]\n",
      " [    0     0     0 ...     2     8     7]\n",
      " [    0     0     0 ...   847 11393     1]]\n",
      "76489     0\n",
      "343634    0\n",
      "358210    0\n",
      "390385    0\n",
      "413360    1\n",
      "         ..\n",
      "265899    0\n",
      "375667    0\n",
      "135531    0\n",
      "150816    1\n",
      "125339    0\n",
      "Name: EI, Length: 288211, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(X_train_EI)\n",
    "print(y_train_EI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(288211, 200)\n",
      "(288211,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_EI.shape)\n",
    "print(y_train_EI.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import keras_tuner\n",
    "import tensorflow.keras.losses\n",
    "import tensorflow.keras.metrics\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    \"\"\" Build the neural network for classifying MBTI dimensions.\n",
    "\n",
    "    Args:\n",
    "    - None.\n",
    "\n",
    "    Returns:\n",
    "    - A compiled keras deep learning model based on the architecture in the paper:\n",
    "    Sakdipat Ontoum, Jonathan H. Chan. Personality Type Based on Myers-Briggs Type Indicator with Text Posting Style by using Traditional and Deep Learning. https://doi.org/10.48550/arXiv.2201.08717.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the layers deep neural network.\n",
    "    embedding_input = layers.Input(shape=(200,))\n",
    "\n",
    "    embedding = layers.Embedding(\n",
    "        input_dim=72000,\n",
    "        output_dim=256\n",
    "    )(embedding_input)\n",
    "\n",
    "    conv1d = layers.Conv1D(\n",
    "        filters=32,\n",
    "        kernel_size=3,\n",
    "        activation=\"relu\"\n",
    "    )(embedding)\n",
    "\n",
    "    conv1d_1 = layers.Conv1D(\n",
    "        filters=32,\n",
    "        kernel_size=3,\n",
    "        activation=\"relu\"    \n",
    "    )(conv1d)\n",
    "\n",
    "    dropout = layers.Dropout(rate=0.1)(conv1d_1)\n",
    "\n",
    "    conv1d_2 = layers.Conv1D(\n",
    "        filters=64,\n",
    "        kernel_size=3,\n",
    "        activation=\"relu\"\n",
    "    )(dropout)\n",
    "\n",
    "    conv1d_3 = layers.Conv1D(\n",
    "        filters=64,\n",
    "        kernel_size=3,\n",
    "        activation=\"relu\"\n",
    "    )(conv1d_2)\n",
    "\n",
    "    dropout_1 = layers.Dropout(rate=0.2)(conv1d_3)\n",
    "\n",
    "    bidirectional_lstm = layers.Bidirectional(layers.LSTM(units=128))(dropout_1)\n",
    "\n",
    "    dense = layers.Dense(\n",
    "        units=1,\n",
    "        activation=\"sigmoid\"\n",
    "    )(bidirectional_lstm)\n",
    "\n",
    "    # Create and compile the model object.\n",
    "    model = Model(inputs=[embedding_input], outputs=[dense])\n",
    "    model.compile(\n",
    "        optimizer=Nadam(learning_rate=1e-3),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "K = keras.backend\n",
    "class OneCycleScheduler(keras.callbacks.Callback):\n",
    "    \"\"\" 1cycle approach to learning rate scheduling when training the deep neural network.\n",
    "\n",
    "    Use this as a callback when training the model in order to use 1cycle learning rate scheduling.\n",
    "    \"\"\"\n",
    "    def __init__(self, iterations, max_rate, start_rate=None,\n",
    "                 last_iterations=None, last_rate=None):\n",
    "        self.iterations = iterations\n",
    "        self.max_rate = max_rate\n",
    "        self.start_rate = start_rate or max_rate / 10\n",
    "        self.last_iterations = last_iterations or iterations // 10 + 1\n",
    "        self.half_iteration = (iterations - self.last_iterations) // 2\n",
    "        self.last_rate = last_rate or self.start_rate / 1000\n",
    "        self.iteration = 0\n",
    "\n",
    "    def _interpolate(self, iter1, iter2, rate1, rate2):\n",
    "        return ((rate2 - rate1) * (self.iteration - iter1)\n",
    "                / (iter2 - iter1) + rate1)\n",
    "    \n",
    "    def on_batch_begin(self, batch, logs):\n",
    "        if self.iteration < self.half_iteration:\n",
    "            rate = self._interpolate(0, self.half_iteration, self.start_rate, self.max_rate)\n",
    "        elif self.iteration < 2 * self.half_iteration:\n",
    "            rate = self._interpolate(self.half_iteration, 2 * self.half_iteration,\n",
    "                                     self.max_rate, self.start_rate)\n",
    "        else:\n",
    "            rate = self._interpolate(2 * self.half_iteration, self.iterations,\n",
    "                                     self.start_rate, self.last_rate)\n",
    "        self.iteration += 1\n",
    "        K.set_value(self.model.optimizer.learning_rate, rate)\n",
    "\n",
    "# The blocks of code below are used for finding the optimal maximum learning rate for 1cycle learning rate scheduling.\n",
    "class ExponentialLearningRate(keras.callbacks.Callback):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "        self.rates = []\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        self.rates.append(K.get_value(self.model.optimizer.learning_rate))\n",
    "        self.losses.append(logs[\"loss\"])\n",
    "        K.set_value(self.model.optimizer.learning_rate, self.model.optimizer.learning_rate * self.factor)\n",
    "\n",
    "def find_learning_rate(model, X, y, epochs=1, batch_size=32, min_rate=10**-5, max_rate=10):\n",
    "    init_weights = model.get_weights()\n",
    "    iterations = math.ceil(len(X) / batch_size) * epochs\n",
    "    factor = np.exp(np.log(max_rate / min_rate) / iterations)\n",
    "    init_lr = K.get_value(model.optimizer.learning_rate)\n",
    "    K.set_value(model.optimizer.learning_rate, min_rate)\n",
    "    exp_lr = ExponentialLearningRate(factor)\n",
    "    history = model.fit(X, y, epochs=epochs, batch_size=batch_size,\n",
    "                        callbacks=[exp_lr])\n",
    "    K.set_value(model.optimizer.learning_rate, init_lr)\n",
    "    model.set_weights(init_weights)\n",
    "    \n",
    "    return exp_lr.rates, exp_lr.losses\n",
    "\n",
    "def plot_lr_vs_loss(rates, losses):\n",
    "    plt.plot(rates, losses)\n",
    "    plt.gca().set_xscale('log')\n",
    "    plt.hlines(min(losses), min(rates), max(rates))\n",
    "    plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 2])\n",
    "    plt.xlabel(\"Learning rate\")\n",
    "    plt.ylabel(\"Loss\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train classifier for Extraversion vs Introversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141/141 [==============================] - 2182s 15s/step - loss: 1.1555 - accuracy: 0.7414\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m ei_classifier \u001b[39m=\u001b[39m build_model()\n\u001b[0;32m      4\u001b[0m rates, losses \u001b[39m=\u001b[39m find_learning_rate(ei_classifier, X_train_EI, y_train_EI, epochs\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, batch_size\u001b[39m=\u001b[39mBATCH_SIZE)\n\u001b[1;32m----> 5\u001b[0m plot_lr_vs_loss(rates, losses)\n",
      "Cell \u001b[1;32mIn[42], line 122\u001b[0m, in \u001b[0;36mplot_lr_vs_loss\u001b[1;34m(rates, losses)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplot_lr_vs_loss\u001b[39m(rates, losses):\n\u001b[1;32m--> 122\u001b[0m     plt\u001b[39m.\u001b[39mplot(rates, losses)\n\u001b[0;32m    123\u001b[0m     plt\u001b[39m.\u001b[39mgca()\u001b[39m.\u001b[39mset_xscale(\u001b[39m'\u001b[39m\u001b[39mlog\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    124\u001b[0m     plt\u001b[39m.\u001b[39mhlines(\u001b[39mmin\u001b[39m(losses), \u001b[39mmin\u001b[39m(rates), \u001b[39mmax\u001b[39m(rates))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 2048\n",
    "\n",
    "ei_classifier = build_model()\n",
    "rates, losses = find_learning_rate(ei_classifier, X_train_EI, y_train_EI, epochs=1, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_lr_vs_loss(rates, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "  5/141 [>.............................] - ETA: 44:45 - loss: 1.0015 - accuracy: 0.6388"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m onecycle \u001b[39m=\u001b[39m OneCycleScheduler(math\u001b[39m.\u001b[39mceil(\u001b[39mlen\u001b[39m(X_train_EI \u001b[39m/\u001b[39m BATCH_SIZE)) \u001b[39m*\u001b[39m N_EPOCHS, max_rate\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m)\n\u001b[0;32m      5\u001b[0m ei_classifier \u001b[39m=\u001b[39m build_model()\n\u001b[1;32m----> 6\u001b[0m ei_classifier\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m      7\u001b[0m     X_train_EI,\n\u001b[0;32m      8\u001b[0m     y_train_EI,\n\u001b[0;32m      9\u001b[0m     validation_data\u001b[39m=\u001b[39;49m(X_test_EI, y_test_EI),\n\u001b[0;32m     10\u001b[0m     batch_size\u001b[39m=\u001b[39;49mBATCH_SIZE,\n\u001b[0;32m     11\u001b[0m     epochs\u001b[39m=\u001b[39;49mN_EPOCHS,\n\u001b[0;32m     12\u001b[0m     callbacks\u001b[39m=\u001b[39;49m[onecycle]\n\u001b[0;32m     13\u001b[0m )\n\u001b[0;32m     14\u001b[0m ei_classifier\u001b[39m.\u001b[39mevaluate(X_test_EI, y_test_EI)\n",
      "File \u001b[1;32mc:\\Users\\stefa\\source\\repos\\myers_briggs_personalities\\venv_myers_briggs_personalities\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\stefa\\source\\repos\\myers_briggs_personalities\\venv_myers_briggs_personalities\\lib\\site-packages\\keras\\engine\\training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1642\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1643\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1644\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1648\u001b[0m ):\n\u001b[0;32m   1649\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1650\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1651\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1652\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\stefa\\source\\repos\\myers_briggs_personalities\\venv_myers_briggs_personalities\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\stefa\\source\\repos\\myers_briggs_personalities\\venv_myers_briggs_personalities\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    877\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    879\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 880\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    882\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    883\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\stefa\\source\\repos\\myers_briggs_personalities\\venv_myers_briggs_personalities\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    909\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    910\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    911\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 912\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_no_variable_creation_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    913\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    914\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    915\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    916\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\stefa\\source\\repos\\myers_briggs_personalities\\venv_myers_briggs_personalities\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    132\u001b[0m   (concrete_function,\n\u001b[0;32m    133\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 134\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    135\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\stefa\\source\\repos\\myers_briggs_personalities\\venv_myers_briggs_personalities\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1741\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1742\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1743\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1744\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1745\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1746\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1747\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1748\u001b[0m     args,\n\u001b[0;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1750\u001b[0m     executing_eagerly)\n\u001b[0;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\stefa\\source\\repos\\myers_briggs_personalities\\venv_myers_briggs_personalities\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    377\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 378\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    379\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    380\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    381\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    382\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    383\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    384\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    385\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    386\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    387\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    390\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    391\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\stefa\\source\\repos\\myers_briggs_personalities\\venv_myers_briggs_personalities\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 20\n",
    "\n",
    "onecycle = OneCycleScheduler(math.ceil(len(X_train_EI / BATCH_SIZE)) * N_EPOCHS, max_rate=0.07)\n",
    "ei_classifier.fit(\n",
    "    X_train_EI,\n",
    "    y_train_EI,\n",
    "    validation_data=(X_test_EI, y_test_EI),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=N_EPOCHS,\n",
    "    callbacks=[onecycle]\n",
    ")\n",
    "ei_classifier.evaluate(X_test_EI, y_test_EI)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_myers_briggs_personalities",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
