{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Development\n",
    "\n",
    "Based on the previous work in the other notebooks, we managed to gain some initial insight for what might be contributing features when it comes to determining an online users MBTI personality factors based on what they posted, such as the overall sentiment of their posts, length of their posts, noun and verb frequency of their posts, etc. Here we try to develop a model with more emphasis on performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE + Word2Vec + Logistic Regression\n",
    "\n",
    "In the paper *Ryan, G.; Katarina, P.; Suhartono, D. MBTI Personality Prediction Using Machine Learning and SMOTE for Balancing Data Based on Statement Sentences. Information 2023, 14, 217*, a combination of SMOTE oversampling, Word2Vec word embeddings and Logistic Regression was used to achieve an average F1 - Score of 0.8337 across the four dimensions of MBTI personalities, namely:\n",
    "- Extraversion vs Introversion\n",
    "- Sensing vs Intuition\n",
    "- Thinking vs Feeling\n",
    "- Judgment vs Perception\n",
    "\n",
    "In this section we will attempt to implement this model from the paper using their methodology."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\stefa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from itertools import zip_longest\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_DATA_PATH = os.path.join(\"data\", \"mbti_1.csv\")\n",
    "\n",
    "def load_csv_data(csv_file_path: str):\n",
    "    \"\"\" Load data from a given csv file into a pandas DataFrame object.\n",
    "\n",
    "    Args:\n",
    "    - csv_file_path (str) - The file path of the csv file containing the desired data to load into a pandas DataFrame object.\n",
    "\n",
    "    Returns:\n",
    "    - data (pandas.DataFrame) - The loaded data as a pandas DataFrame object.\n",
    "    \"\"\"\n",
    "    assert csv_file_path.endswith(\".csv\")\n",
    "\n",
    "    return pd.read_csv(csv_file_path)\n",
    "\n",
    "def get_online_posts_df(df: pd.DataFrame, type_col: str = \"type\", posts_col: str = \"posts\"):\n",
    "    \"\"\" Takes in an input pandas DataFrame, each row having a (user_personality_type, user_recent_50_comments) schema and user_recent_50_comments is a '|||' delimited string.\n",
    "    Outputs a DataFrame object where each row is a (user_personality_type, user_comment), where each row no contains exactly one comment.\n",
    "\n",
    "    Args:\n",
    "    - df (pandas.DataFrame) - The input pandas DataFrame object, where each row follows the schema (user_personality_type, user_recent_50_comments).\n",
    "    - type_col (str) - The column name of the input DataFrame's personality type column, which contains the user's personality type.\n",
    "    - posts_col (str) - The column name of the input DataFrame's posts column, which contains the user's recent 50 comments/online posts\n",
    "\n",
    "    Returns:\n",
    "    - output_df (pandas.DataFrame) - The output pandas DataFrame object, where each row follows the schema (user_personality_type, user_comment).\n",
    "    \"\"\"\n",
    "    online_posts_dict = {\n",
    "        type_col: [],\n",
    "        posts_col: []\n",
    "    }\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        personality_type = row[type_col]\n",
    "        recent_50_comments = row[posts_col].split(\"|||\")\n",
    "\n",
    "        online_posts_dict[type_col] += [personality_type] * len(recent_50_comments)\n",
    "        online_posts_dict[posts_col] += recent_50_comments\n",
    "\n",
    "    columns = online_posts_dict.keys()\n",
    "    values = list(zip_longest(*online_posts_dict.values()))\n",
    "    output_df = pd.DataFrame(values, columns=columns)\n",
    "\n",
    "    return output_df\n",
    "\n",
    "def get_personality_dimensions(personality_dataset: pd.DataFrame, personalities_col_name: str = \"type\"):\n",
    "    \"\"\" Return a dictionary containing the personality dimensions from the given pandas DataFrame containing Myers Briggs personalities.\n",
    "\n",
    "    Args:\n",
    "    - personality_dataset (pandas.DataFrame) - The pandas DataFrame object containing the Myers Briggs personalities data.\n",
    "    - personalities_col_name (str) - The name of the column which contains the Myers Briggs personalities in the given DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - personality_factors_dict (dict) - A dictionary containing the number personality dimensions, split into Extraversion vs Introversion, Sensing vs Intuition, Thinking vs Feeling, Judging vs Perceiving.\n",
    "    \"\"\"\n",
    "    personality_factors_dict = {\n",
    "        \"e_vs_i\": [],\n",
    "        \"s_vs_n\": [],\n",
    "        \"t_vs_f\": [],\n",
    "        \"j_vs_p\": []\n",
    "    }\n",
    "\n",
    "    for index, row in personality_dataset.iterrows():\n",
    "        personality_type_str = row[personalities_col_name]\n",
    "        e_vs_i = personality_type_str[0]\n",
    "        s_vs_n = personality_type_str[1]\n",
    "        t_vs_f = personality_type_str[2]\n",
    "        j_vs_p = personality_type_str[3]\n",
    "\n",
    "        personality_factors_dict[\"e_vs_i\"].append(e_vs_i)\n",
    "        personality_factors_dict[\"s_vs_n\"].append(s_vs_n)\n",
    "        personality_factors_dict[\"t_vs_f\"].append(t_vs_f)\n",
    "        personality_factors_dict[\"j_vs_p\"].append(j_vs_p)\n",
    "\n",
    "    return personality_factors_dict  \n",
    "\n",
    "def preprocess_df(df: pd.DataFrame):\n",
    "    \"\"\" Helper function for preprocessing the input pandas dataframe by helping clean up input user posts.\n",
    "\n",
    "    Args:\n",
    "    - df (pd.DataFrame) - The input dataframe object containing the user posts to clean up.\n",
    "\n",
    "    Returns:\n",
    "    - df (pd.DataFrame) - The given dataframe object after cleaning has been applied to the user posts data.\n",
    "    \"\"\"   \n",
    "\n",
    "    # Make each data record a user post rather than a user's top 50 posts.\n",
    "    df = get_online_posts_df(df)\n",
    "\n",
    "    # Extract the labels across the four dimensions of MBTI personalities corresponding to each post\n",
    "    personality_factors_per_post_dict = get_personality_dimensions(df)\n",
    "    columns = personality_factors_per_post_dict.keys()\n",
    "    values = list(zip_longest(*personality_factors_per_post_dict.values()))\n",
    "    personality_factors_per_post = pd.DataFrame(values, columns=columns)\n",
    "    df = pd.concat([personality_factors_per_post, df], axis=1)\n",
    "\n",
    "    # Remove urls\n",
    "    regex_pattern = r\"((?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’])))\"\n",
    "    df[\"posts_no_url\"] = df[\"posts\"].str.replace(regex_pattern, \"\").str.strip()\n",
    "\n",
    "    # Keep the End Of Sentence characters\n",
    "    df[\"posts_no_url\"] = df[\"posts_no_url\"].apply(lambda x: re.sub(r'\\.', ' EOSTokenDot ', str(x) + \" \"))\n",
    "    df[\"posts_no_url\"] = df[\"posts_no_url\"].apply(lambda x: re.sub(r'\\?', ' EOSTokenQuest ', str(x) + \" \"))\n",
    "    df[\"posts_no_url\"] = df[\"posts_no_url\"].apply(lambda x: re.sub(r'!', ' EOSTokenExs ', str(x) + \" \"))\n",
    "    \n",
    "    # Strip Punctation\n",
    "    df[\"posts_no_url\"] = df[\"posts_no_url\"].apply(lambda x: re.sub(r'[\\.+]', \".\", str(x)))\n",
    "\n",
    "    # Remove multiple fullstops\n",
    "    # df[\"posts_no_url\"] = df[\"posts_no_url\"].apply(lambda x: re.sub(r'[^\\w\\s]','', str(x)))\n",
    "\n",
    "    # Remove Non-words\n",
    "    # df[\"posts_no_url\"] = df[\"posts_no_url\"].apply(lambda x: re.sub(r'[^a-zA-Z\\s]','', str(x)))\n",
    "\n",
    "    # Convert posts to lowercase\n",
    "    df[\"posts_no_url\"] = df[\"posts_no_url\"].apply(lambda x: str(x).lower())\n",
    "\n",
    "    # Remove multiple letter repeating words\n",
    "    # df[\"posts_no_url\"] = df[\"posts_no_url\"].apply(lambda x: re.sub(r'([a-z])\\1{2,}[\\s|\\w]*','', str(x)))\n",
    "    \n",
    "    # Strip trailing whitespaces\n",
    "    df[\"posts_no_url\"] = df[\"posts_no_url\"].apply(lambda x: str(x).strip())\n",
    "\n",
    "    # Remove rows with no text\n",
    "    df[\"posts_no_url\"] = df[\"posts_no_url\"].replace('', np.nan)\n",
    "    df[\"posts_no_url\"] = df[\"posts_no_url\"].replace('nan', np.nan)\n",
    "    df[\"posts_no_url\"] = df[\"posts_no_url\"].replace(\"'\", np.nan)\n",
    "    df[\"posts_no_url\"] = df[\"posts_no_url\"].replace(\"''\", np.nan)\n",
    "    df[\"posts_no_url\"] = df[\"posts_no_url\"].replace('\"', np.nan)\n",
    "    df[\"posts_no_url\"] = df[\"posts_no_url\"].replace('\"\"', np.nan)\n",
    "    df.dropna(subset=[\"posts_no_url\"], inplace=True)\n",
    "\n",
    "    # Tokenize posts\n",
    "    df[\"posts_no_url_tokens\"] = df[\"posts_no_url\"].apply(wordpunct_tokenize)\n",
    "\n",
    "    # Remove stop words\n",
    "    df[\"posts_no_url_tokens_no_stop\"] = df[\"posts_no_url_tokens\"].apply(lambda x: [token for token in x if token not in stop_words])\n",
    "\n",
    "    # Lemmatize posts\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    df[\"posts_no_url_tokens_no_stop\"] = df[\"posts_no_url_tokens_no_stop\"].apply(lambda x: [lemmatizer.lemmatize(token) for token in x])\n",
    "\n",
    "    return df[[\"e_vs_i\", \"s_vs_n\", \"t_vs_f\", \"j_vs_p\", \"type\", \"posts\", \"posts_no_url\", \"posts_no_url_tokens_no_stop\"]]\n",
    "\n",
    "def binarize_targets(df: pd.DataFrame):\n",
    "    \"\"\" Apply 0/1 labels to the input classes in the df.\n",
    "\n",
    "    Args:\n",
    "    - df (pd.DataFrame) - The input dataframe object containing the classes to numerically label.\n",
    "\n",
    "    Returns:\n",
    "    - df (pd.DataFrame) - The given dataframe object after numerically labelling classes.\n",
    "    \"\"\"\n",
    "    binary_map = {\n",
    "        'I': 0, \n",
    "        'E': 1, \n",
    "        'N': 0, \n",
    "        'S': 1, \n",
    "        'F': 0, \n",
    "        'T': 1, \n",
    "        'J': 0, \n",
    "        'P': 1\n",
    "    }\n",
    "\n",
    "    df[\"EI\"] = df[\"e_vs_i\"].apply(lambda x: binary_map[str(x)])\n",
    "    df[\"SN\"] = df[\"s_vs_n\"].apply(lambda x: binary_map[str(x)])\n",
    "    df[\"TF\"] = df[\"t_vs_f\"].apply(lambda x: binary_map[str(x)])\n",
    "    df[\"JP\"] = df[\"j_vs_p\"].apply(lambda x: binary_map[str(x)])\n",
    "\n",
    "    df[\"target_vec\"] = df.apply(lambda x: [\n",
    "        x[\"EI\"],\n",
    "        x[\"SN\"],\n",
    "        x[\"TF\"],\n",
    "        x[\"JP\"]\n",
    "    ], axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def preprocess_dataset(csv_file_path: str = CSV_DATA_PATH):\n",
    "    \"\"\" Load and preprocess data to be used for model development.\n",
    "\n",
    "    Args:\n",
    "    - csv_file_path (str) - The file path of the csv file containing the desired data to load into a pandas DataFrame object.\n",
    "\n",
    "    Returns:\n",
    "    - numpy arrays of training and test split data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Apply preprocessing to input user posts\n",
    "    df = load_csv_data(csv_file_path)\n",
    "    df = preprocess_df(df)\n",
    "\n",
    "    # Preprocess target labels into a binary vector\n",
    "    df = binarize_targets(df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stefa\\AppData\\Local\\Temp\\ipykernel_12024\\3478628864.py:99: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df[\"posts_no_url\"] = df[\"posts\"].str.replace(regex_pattern, \"\").str.strip()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>e_vs_i</th>\n",
       "      <th>s_vs_n</th>\n",
       "      <th>t_vs_f</th>\n",
       "      <th>j_vs_p</th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "      <th>posts_no_url</th>\n",
       "      <th>posts_no_url_tokens_no_stop</th>\n",
       "      <th>EI</th>\n",
       "      <th>SN</th>\n",
       "      <th>TF</th>\n",
       "      <th>JP</th>\n",
       "      <th>target_vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>J</td>\n",
       "      <td>INFJ</td>\n",
       "      <td>enfp and intj moments  https://www.youtube.com...</td>\n",
       "      <td>enfp and intj moments    sportscenter not top ...</td>\n",
       "      <td>[enfp, intj, moment, sportscenter, top, ten, p...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>J</td>\n",
       "      <td>INFJ</td>\n",
       "      <td>What has been the most life-changing experienc...</td>\n",
       "      <td>what has been the most life-changing experienc...</td>\n",
       "      <td>[life, -, changing, experience, life, eostoken...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>J</td>\n",
       "      <td>INFJ</td>\n",
       "      <td>http://www.youtube.com/watch?v=vXZeYwwRDw8   h...</td>\n",
       "      <td>on repeat for most of today eostokendot</td>\n",
       "      <td>[repeat, today, eostokendot]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>J</td>\n",
       "      <td>INFJ</td>\n",
       "      <td>May the PerC Experience immerse you.</td>\n",
       "      <td>may the perc experience immerse you eostokendot</td>\n",
       "      <td>[may, perc, experience, immerse, eostokendot]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>J</td>\n",
       "      <td>INFJ</td>\n",
       "      <td>The last thing my INFJ friend posted on his fa...</td>\n",
       "      <td>the last thing my infj friend posted on his fa...</td>\n",
       "      <td>[last, thing, infj, friend, posted, facebook, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  e_vs_i s_vs_n t_vs_f j_vs_p  type  \\\n",
       "2      I      N      F      J  INFJ   \n",
       "3      I      N      F      J  INFJ   \n",
       "4      I      N      F      J  INFJ   \n",
       "5      I      N      F      J  INFJ   \n",
       "6      I      N      F      J  INFJ   \n",
       "\n",
       "                                               posts  \\\n",
       "2  enfp and intj moments  https://www.youtube.com...   \n",
       "3  What has been the most life-changing experienc...   \n",
       "4  http://www.youtube.com/watch?v=vXZeYwwRDw8   h...   \n",
       "5               May the PerC Experience immerse you.   \n",
       "6  The last thing my INFJ friend posted on his fa...   \n",
       "\n",
       "                                        posts_no_url  \\\n",
       "2  enfp and intj moments    sportscenter not top ...   \n",
       "3  what has been the most life-changing experienc...   \n",
       "4            on repeat for most of today eostokendot   \n",
       "5    may the perc experience immerse you eostokendot   \n",
       "6  the last thing my infj friend posted on his fa...   \n",
       "\n",
       "                         posts_no_url_tokens_no_stop  EI  SN  TF  JP  \\\n",
       "2  [enfp, intj, moment, sportscenter, top, ten, p...   0   0   0   0   \n",
       "3  [life, -, changing, experience, life, eostoken...   0   0   0   0   \n",
       "4                       [repeat, today, eostokendot]   0   0   0   0   \n",
       "5      [may, perc, experience, immerse, eostokendot]   0   0   0   0   \n",
       "6  [last, thing, infj, friend, posted, facebook, ...   0   0   0   0   \n",
       "\n",
       "     target_vec  \n",
       "2  [0, 0, 0, 0]  \n",
       "3  [0, 0, 0, 0]  \n",
       "4  [0, 0, 0, 0]  \n",
       "5  [0, 0, 0, 0]  \n",
       "6  [0, 0, 0, 0]  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = preprocess_dataset()\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following preprocessing steps were applied to the dataset:\n",
    "- Converting letters to lowercase\n",
    "- Removing links\n",
    "- Removing punctuations\n",
    "- Removing stopwords\n",
    "\n",
    "Lematization was also applied after the above steps have been conducted, and the resulting text for each post tokenized."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word Embeddings and Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Word2Vec model\n",
    "w2w_model = Word2Vec(sentences=df[\"posts_no_url_tokens_no_stop\"], vector_size=100, window=5, min_count=5, epochs=50)\n",
    "\n",
    "def get_train_test_split(training_fraction: float, df: pd.DataFrame, mbti_dim: str):\n",
    "    \"\"\" Get a training and test dataset split.\n",
    "\n",
    "    Args:\n",
    "    - training_proportion (float) - The fraction of the dataset to be used as training data.\n",
    "    - df (pd.DataFrame) - The pandas dataframe object containing the data to be split into train and test sets.\n",
    "    - mbti_dim (str) - The column name of the target MBTI personality dimension.\n",
    "\n",
    "    Returns:\n",
    "    - Train and test datasets where the target variable is the desired MBTI personality dimension (mbti_dim).\n",
    "    \"\"\"\n",
    "\n",
    "    # Split into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df[\"posts_no_url_tokens_no_stop\"], df[mbti_dim], test_size=(1 - training_fraction), random_state=42)\n",
    "\n",
    "    X_train = np.array([vectorize(post) for post in X_train])\n",
    "    X_test = np.array([vectorize(post) for post in X_test])\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def vectorize(post: list):\n",
    "    \"\"\" Vectorize a sentence to a vector representation using Word2Vec CBOW.\n",
    "\n",
    "    Args:\n",
    "    - post (str) - A post to vectorize. Here, the post is passed into the function as a list of word tokens.\n",
    "\n",
    "    Returns:\n",
    "    - Word2Vec word embedding (CBOW).\n",
    "    \"\"\"\n",
    "\n",
    "    # Vectorize sentence\n",
    "    words_vector = [w2w_model.wv[token] for token in post if token in w2w_model.wv]\n",
    "\n",
    "    if len(words_vector) == 0:\n",
    "        return np.zeros(100)\n",
    "    \n",
    "    words_vector = np.array(words_vector)\n",
    "    return words_vector.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.0835064  -0.0383866  -0.71517909 ... -0.34353155 -0.45407093\n",
      "   0.5426448 ]\n",
      " [-0.54806298 -0.3403216   0.07760847 ...  0.0341871   0.0330039\n",
      "   1.4627912 ]\n",
      " [ 0.40651768 -0.2537263  -0.18573768 ... -0.63773656 -0.03819894\n",
      "   1.45130706]\n",
      " ...\n",
      " [-0.07754824 -0.78628856 -0.76521254 ... -0.42973948 -0.41955894\n",
      "  -0.01212895]\n",
      " [ 0.64870238  0.26647463  0.14806095 ... -1.31521332  1.23697543\n",
      "   0.68142593]\n",
      " [-0.15476277 -0.29810882 -0.19611789 ... -0.8667978  -0.41394207\n",
      "   1.28957248]]\n"
     ]
    }
   ],
   "source": [
    "X_train_EI, X_test_EI, y_train_EI, y_test_EI = get_train_test_split(0.7, df, \"EI\") \n",
    "print(X_train_EI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.17530116  0.31277245 -0.11343044 -0.69663709  0.01143652  0.0299514\n",
      " -0.10146542  0.09827747  0.55080926  0.53086746 -0.4468869   0.20318441\n",
      " -0.51653618 -0.14168708  0.10792978  0.26029581 -0.01586989 -0.47865528\n",
      "  0.24886933  0.39818797 -0.50121671  0.5289852  -0.14484823  0.8994171\n",
      "  0.10309625  0.38239402  0.61306077  0.41539803 -0.28249019 -0.42193329\n",
      "  0.07537617 -0.03833069 -0.57481825  1.19181073 -0.00124663 -0.36107755\n",
      " -0.01071989  0.84230459 -0.12940501 -0.6351015   0.54436874  0.25752887\n",
      "  0.20340458 -0.84556466  0.84865105  0.63278168  0.13476034 -0.52210855\n",
      "  0.14478211  0.04437743 -0.07853041 -0.3531898  -0.57878882  0.040739\n",
      " -0.41626593 -0.08038055  0.0222159   0.38431546  0.08259805  0.11167838\n",
      "  0.00862952 -0.33293933 -0.23189045 -0.08287065 -0.29605269 -0.28030172\n",
      " -0.06947268 -0.33559802 -0.19908157  0.53771794  0.17793454 -0.11064775\n",
      "  0.15404843  0.3413114   0.08470335 -0.01158373 -0.38087112 -0.11531255\n",
      "  0.34635121 -0.14189805  0.83237898 -0.90523565 -0.13614206 -0.12214974\n",
      "  0.45154056  0.56152225 -0.68639106  0.11130905 -0.41024709 -0.51531237\n",
      " -0.16149949 -0.31252646  0.22895904  0.0625426  -0.31831327 -0.86519706\n",
      "  0.2131062  -0.40154022 -0.08701306 -0.34517151]\n"
     ]
    }
   ],
   "source": [
    "print(X_test_EI[160])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create functionality that can split out user posts dataset into train and test sets, and vectorize the text data to be used as model inputs as Word2Vec word embeddings. The benefit with using word embeddings is that we can represent words in a numerical format (ML/DL models can only take in numerical input) that allows words with similar meanings to have the same representation, i.e. smilarity of meaning between words is captured in the word embeddings."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_LR():\n",
    "    \"\"\" Create a simple Logistic Regression model on the given training data. \n",
    "    SMOTE oversampling will also performed when this model is fitted to training data.\n",
    "\n",
    "    Args:\n",
    "    - X (iterable) - Input training examples.\n",
    "    - y (iterable) - Training target labels.\n",
    "\n",
    "    Returns:\n",
    "    - The Logistic Regression model to be fitted onto training data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Instatiate SMOTE over-sampler and Logistic Regression model.\n",
    "    sampler = SMOTE(random_state=42)\n",
    "    lr = LogisticRegression()\n",
    "\n",
    "    # Build pipeline/model.\n",
    "    pipeline = Pipeline([\n",
    "        (\"sampler\", sampler),\n",
    "        (\"lr\", lr)\n",
    "    ])\n",
    "\n",
    "    return pipeline\n",
    "\n",
    "def train_model(X, y, model=model_LR):\n",
    "    \"\"\" Train the input model on the given training data.\n",
    "\n",
    "    Args:\n",
    "    - X (iterable) - Input training examples.\n",
    "    - y (iterable) - Training target labels.\n",
    "\n",
    "    Returns:\n",
    "    - The model fitted to the training data.\n",
    "    \"\"\"\n",
    "    model.fit(X, y)\n",
    "\n",
    "    return model\n",
    "\n",
    "def evaluate(X, y, model):\n",
    "    \"\"\" Evaluate the given trained model on the test data.\n",
    "\n",
    "    Args:\n",
    "    - X (iterable) - Input test examples.\n",
    "    - y (iterable) - Test target labels.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(X)\n",
    "    print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets for each MBTI dimension.\n",
    "X_train_EI, X_test_EI, y_train_EI, y_test_EI = get_train_test_split(0.7, df, \"EI\")\n",
    "X_train_SN, X_test_SN, y_train_SN, y_test_SN = get_train_test_split(0.7, df, \"SN\")\n",
    "X_train_TF, X_test_TF, y_train_TF, y_test_TF = get_train_test_split(0.7, df, \"TF\") \n",
    "X_train_JP, X_test_JP, y_train_JP, y_test_JP = get_train_test_split(0.7, df, \"JP\")   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_myers_briggs_personalities",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
